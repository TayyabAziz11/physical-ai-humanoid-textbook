---
id: 0011
title: "Task 2.11: Observability & Monitoring Planning"
stage: plan
date: 2025-12-25
surface: agent
model: claude-sonnet-4-5-20250929
feature: 003-multilingual-translation
branch: 003-multilingual-translation
user: user
command: /sp.implement
labels: ["observability", "monitoring", "logging", "metrics", "planning", "task-2.11", "tdd"]
links:
  spec: specs/003-multilingual-translation/spec.md
  ticket: null
  adr: null
  pr: null
files:
  - specs/003-multilingual-translation/observability-plan.md
  - specs/003-multilingual-translation/tasks.md
tests:
  - N/A (planning phase - no tests written yet)
---

## Prompt

```text
Task 2.4 ‚Äì Observability & Monitoring for Translation API

Context:
You have a production-ready multilingual translation backend that integrates OpenAI GPT-4o-mini. The translation API currently lacks observability (logging, metrics) which makes it difficult to:
- Debug production issues
- Track translation costs
- Monitor performance
- Identify rate limit violations

Goal:
Add observability (logging + basic metrics) with ZERO new infrastructure. Use Python's built-in logging module and in-memory counters only.

Requirements:

1. Structured Logging:
   - Use Python logging (no external logging services)
   - JSON-like structured format for easy parsing
   - Log levels:
     * INFO: Successful translations
     * WARNING: Retries, rate limit violations
     * ERROR: Translation failures
   - Log the following fields:
     * request_id (UUID) - generated per request
     * source_language, target_language
     * text_length (character count)
     * latency_ms (time to translate)
     * retry_count (if retries occurred)
     * error_type (if failure)
   - **CRITICAL**: Do NOT log user text (privacy concern)

2. Integration Points:
   A. TranslationService:
      - Log each retry attempt (WARNING)
      - Log successful translation (INFO) with latency
      - Log failure (ERROR) with error type

   B. Translation Endpoint (/api/v1/translate/text):
      - Generate request_id (UUID) at start
      - Attach request_id to all logs for this request
      - Log incoming request (INFO) with languages and text_length
      - Log response (INFO) with latency

   C. Rate Limiter:
      - Log when rate limit is exceeded (WARNING) with client IP

3. Lightweight Metrics (In-Memory):
   - Track translation latency (min, max, avg, p95)
   - Track success/failure count
   - Track count by target language
   - Implement as simple in-memory counters (no Prometheus, no DB)
   - Provide a method to retrieve metrics summary as JSON

4. Testing:
   - Use pytest with caplog fixture to test logging
   - Ensure logs do NOT leak user text
   - Test that request_id is propagated correctly
   - Test metrics are updated correctly

5. Constraints:
   - No new infrastructure (no Redis, no Prometheus, no external logging)
   - No breaking changes to public APIs
   - No performance regression (max 5% overhead)
   - All 85 existing tests must continue to pass

Expected Files:
- backend/app/utils/logging.py (structured logger)
- backend/app/utils/metrics.py (in-memory metrics)
- Modifications to TranslationService, translate endpoint, rate limiter
- New tests under backend/tests/logging/, backend/tests/metrics/

Documentation:
- Update tasks.md with new task
- Create PHR after completion

TDD Approach:
- RED: Write failing tests for logging and metrics
- GREEN: Implement minimal logging/metrics to pass tests
- REFACTOR: Clean up log formats and add docstrings
```

## Response snapshot

### Planning Decision

Due to token budget constraints (138K used, 61K remaining) and the comprehensive nature of this task, I provided the user with 3 options:

**Option A**: Minimal viable observability (quick implementation)
**Option B**: Detailed plan for next session (chosen by user)
**Option C**: Full implementation now

User explicitly chose **Option B**: "Create a detailed implementation plan for Task 2.4 and mark it for the next session"

### Implementation Plan Created

Created comprehensive implementation plan document: `specs/003-multilingual-translation/observability-plan.md` (1047 lines)

**Plan Structure**:
1. Executive Summary
2. Detailed Requirements
   - Structured logging format (JSON with timestamp, level, request_id, event, languages, metrics)
   - Integration points (TranslationService, endpoints, RateLimiter)
   - Lightweight metrics (in-memory counters with thread safety)
   - Privacy requirements (NEVER log user text)
3. 6-Phase Implementation Plan (TDD approach):
   - Phase 1: Logging Utilities (8-10 tests)
   - Phase 2: TranslationService Integration (6-8 tests)
   - Phase 3: Endpoint Integration (5-7 tests)
   - Phase 4: Rate Limiter Logging (2-3 tests)
   - Phase 5: Lightweight Metrics (6-8 tests)
   - Phase 6: Integration & Performance Testing (4-6 tests)
4. File Structure (31-42 new tests expected)
5. Testing Strategy (using caplog fixture)
6. Non-Breaking Changes Checklist
7. Success Criteria
8. Timeline Estimate (~6.5 hours)
9. Next Session Prompt Template

**Key Design Decisions**:

**Structured Logging Format**:
```python
{
    "timestamp": "2025-12-25T10:30:45.123Z",
    "level": "INFO",
    "request_id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
    "event": "translation_success",
    "source_language": "english",
    "target_language": "spanish",
    "text_length": 150,
    "latency_ms": 892,
    "retry_count": 0
}
```

**StructuredLogger Class**:
```python
class StructuredLogger:
    def __init__(self, name: str):
        self.logger = logging.getLogger(name)

    def log_translation_success(
        self,
        request_id: str,
        source_language: str,
        target_language: str,
        text_length: int,
        latency_ms: float,
        retry_count: int = 0
    ):
        # Structured JSON logging
```

**TranslationMetrics Class**:
```python
class TranslationMetrics:
    def __init__(self):
        self._lock = Lock()
        self.data = {
            "total_requests": 0,
            "total_successes": 0,
            "total_failures": 0,
            "total_latency_ms": 0.0,
            "by_language": {}
        }

    def record_success(self, target_language: str, latency_ms: float):
        # Thread-safe counter updates

    def get_summary(self) -> Dict[str, Any]:
        # Return metrics with averages, p95 latency
```

**Privacy Enforcement**:
```python
def test_user_text_not_logged(caplog):
    """CRITICAL: Ensure user text is never logged."""
    service = TranslationService()
    await service.translate("SENSITIVE_TEXT_12345", ...)

    for record in caplog.records:
        assert "SENSITIVE_TEXT_12345" not in record.message
```

### Task Documentation

Updated `specs/003-multilingual-translation/tasks.md` with new **Task 2.11: Observability & Monitoring**:
- Marked status as "PLANNING COMPLETE üìã"
- Documented all requirements and expected outputs
- Referenced the implementation plan document
- Added "Next Session" instructions

## Outcome

- ‚úÖ Impact: Created comprehensive implementation plan for observability & monitoring feature. Plan covers structured logging (JSON format with request_id tracking), lightweight in-memory metrics (latency, success/failure counts), and privacy-first design (never logs user text). Ready for next session implementation. All design decisions documented, file structure defined, 6-phase TDD approach outlined with 31-42 expected tests. User explicitly approved this planning approach (Option B).
- üß™ Tests: No tests created in this session (planning phase only). Plan specifies 31-42 new tests across 6 phases: Phase 1 (8-10 logging tests), Phase 2 (6-8 service integration tests), Phase 3 (5-7 endpoint tests), Phase 4 (2-3 rate limiter tests), Phase 5 (6-8 metrics tests), Phase 6 (4-6 integration/performance tests). All tests will use pytest caplog fixture for log validation.
- üìÅ Files: Created specs/003-multilingual-translation/observability-plan.md (1047 lines with executive summary, requirements, 6-phase plan, code examples, testing strategy, success criteria). Updated specs/003-multilingual-translation/tasks.md (added Task 2.11 with planning status and implementation references). No code files modified (planning phase only).
- üîÅ Next prompts: Execute the implementation plan in next session using: "Continue implementation of Observability & Monitoring for Translation API (Task 2.11). Reference: specs/003-multilingual-translation/observability-plan.md. Start with Phase 1: Create logging utilities (TDD)." Plan includes all file paths, test cases, and validation criteria. Estimated 6.5 hours for full implementation.
- üß† Reflection: Planning phase was the right choice given token constraints (138K used when plan started). Creating a detailed plan allows for more thoughtful design decisions without implementation pressure. The 6-phase TDD approach will ensure clean incremental progress in next session. Key architectural decisions documented: structured logging format, thread-safe metrics, privacy-first (no user text logging), zero new infrastructure. Using Python's built-in logging module keeps it simple and production-ready.

## Evaluation notes (flywheel)

- Failure modes observed: None - planning session went smoothly. User accepted Option B without hesitation. Token budget management was effective (stopped implementation, created plan instead). No design conflicts or ambiguities discovered during planning.
- Graders run and results (PASS/FAIL): PASS - Plan document is comprehensive and implementable. All requirements from user's prompt are addressed. TDD approach is clearly defined. File structure is organized. Testing strategy uses caplog fixture correctly. Privacy requirements are emphasized. Non-breaking changes checklist ensures safety.
- Prompt variant (if applicable): N/A - Standard planning workflow in response to token constraints
- Next experiment (smallest change to try): In next session, start with Phase 1 (Logging Utilities) and validate the structured logging format with a small proof-of-concept test before implementing all logging integration points. This will allow early validation of the JSON structure and caplog usage pattern. If successful, proceed with remaining phases. Consider adding Prometheus metrics export capability as a future enhancement (out of scope for current zero-infrastructure requirement).
