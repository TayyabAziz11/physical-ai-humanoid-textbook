---
id: chapter-1-vla-intro
title: "Chapter 1 - Introduction to VLA"
sidebar_label: "Chapter 1: VLA Intro"
---

import ChapterActionsBar from '@site/src/components/learning/ChapterActionsBar';

# باب 1: Vision-Language-Action کا تعارف

<ChapterActionsBar />

## تعارف: AI اور Robotics کا سنگم

تصور کریں کہ آپ ہیومنائیڈ روبوٹ سے کہتے ہیں: "براہ کرم باورچی خانے کے کاؤنٹر سے سرخ کپ لے آئیں۔"

اس سادہ درخواست کے کام کرنے کے لیے، روبوٹ کو ضرورت ہے:

1. **زبان سمجھنا**: "باورچی خانے کے کاؤنٹر سے سرخ کپ لے آئیں" کو parse کرنا
2. **دنیا دیکھنا**: اشیاء کی شناخت، جگہوں میں navigate کرنا، رکاوٹوں سے بچنا
3. **عمل کرنا**: راستے کی منصوبہ بندی، کپ پکڑنا، اسے محفوظ طریقے سے لے جانا

روایتی robotics ان صلاحیتوں کو آزاد نظاموں میں الگ کرتا ہے:

- تقریر کی شناخت → قدرتی زبان کی پروسیسنگ
- کمپیوٹر ویژن → آبجیکٹ ڈیٹیکشن
- حرکت کی منصوبہ بندی → کنٹرول سسٹم

ہر نظام آزادانہ طور پر بنایا اور تربیت یافتہ ہوتا ہے، پھر نازک integration code کے ساتھ جوڑا جاتا ہے۔

**Vision-Language-Action (VLA)** models ایک paradigm shift کی نمائندگی کرتے ہیں: ایک **واحد neural network** جو براہ راست pixels اور text سے robot actions کی طرف map کرتا ہے۔

**یہ کیوں اہم ہے:**

- **End-to-End Learning**: پوری pipeline کو ایک ساتھ تربیت دیں، الگ حصوں کو نہیں
- **Emergent Behaviors**: Model وہ shortcuts سیکھتا ہے جو انسانوں نے کبھی واضح طور پر program نہیں کیے
- **Data Efficiency**: robotics کے لیے internet-scale language اور vision data سے فائدہ اٹھائیں
- **Generalization**: ایک model بہت سے کاموں اور ماحول میں کام کرتا ہے

اس باب میں، آپ سیکھیں گے کہ کیسے Google کے RT-1 اور RT-2 جیسے VLA models robots کو scripted machines سے adaptive agents میں تبدیل کر رہے ہیں۔

---

## VLA Models کیا ہیں؟

**Vision-Language-Action (VLA)** models neural networks ہیں جو **visual observations** (camera images) اور **language instructions** (text commands) کو input کے طور پر لیتے ہیں، اور **robot actions** (joint positions، gripper states) output کرتے ہیں۔

### Traditional Pipeline (پرانا طریقہ)

```
User: "سیب اٹھائیں"
    ↓
Speech-to-Text (Whisper) → "سیب اٹھائیں"
    ↓
NLP (GPT-4) → Intent: PICK_OBJECT, Target: "سیب"
    ↓
Object Detection (YOLO) → Bounding box: [x=120, y=340, w=80, h=100]
    ↓
Motion Planning (MoveIt) → Joint trajectory: [θ₁, θ₂, ..., θ₇]
    ↓
Robot حرکت کو execute کرتا ہے
```

**مسائل:**

- ہر جزو آزادانہ طور پر ناکام ہوتا ہے (detection چھوٹ جاتا ہے، planner پھنس جاتا ہے)
- غلطیاں pipeline کے ذریعے compound ہوتی ہیں
- کوئی shared learning نہیں—vision planning میں مدد نہیں کرتا، اور vice versa

### VLA Pipeline (نیا طریقہ)

```
User: "سیب اٹھائیں"
    ↓
VLA Model (RT-2)
  Inputs: Camera image + "سیب اٹھائیں"
  Output: [Joint deltas: Δθ₁, Δθ₂, ..., Δθ₇, gripper_open: False]
    ↓
Robot عمل کو براہ راست execute کرتا ہے
```

**فوائد:**

- **واحد model** پوری mapping سیکھتا ہے
- **End-to-end optimization** حتمی کام کی غلطی کو کم سے کم کرتی ہے، درمیانی metrics کو نہیں
- **Implicit reasoning** ابھرتا ہے (مثلاً، واضح obstacle detection کے بغیر "رکاوٹ سے بچیں")

---

## اہم VLA Architectures

### 1. RT-1 (Robotics Transformer 1)

**Google Research کی طرف سے جاری (2022)**

RT-1 ایک transformer-based model ہے جو حقیقی باورچی خانوں اور دفاتر میں اکٹھے کیے گئے 130,000 robot demonstrations پر تربیت یافتہ ہے۔

**فن تعمیر:**

```
┌──────────────────────────────────────┐
│  Camera Images (6 views)             │  ← Visual input
│  [224x224x3 فی camera]              │
└───────────┬──────────────────────────┘
            ↓
    ┌───────────────┐
    │  EfficientNet │  ← Image encoder
    │  (pretrained) │
    └───────┬───────┘
            ↓
    [Visual tokens: 512-dim]
            ↓
┌──────────────────────────────────────┐
│  Language Instruction                │  ← Language input
│  "نیلا بلاک اٹھائیں"                │
└───────────┬──────────────────────────┘
            ↓
    ┌───────────────┐
    │  USE Encoder  │  ← Universal Sentence Encoder
    └───────┬───────┘
            ↓
    [Language tokens: 512-dim]
            ↓
    ┌─────────────────────────┐
    │  Transformer Decoder    │  ← Attention mechanism
    │  (8 layers, 128 heads)  │
    └───────────┬─────────────┘
                ↓
    [Action tokens: 11-dim]
                ↓
    ┌───────────────────────────────────┐
    │  Robot Actions (ہر 3 Hz)        │
    │  [x, y, z, roll, pitch, yaw,     │
    │   gripper, terminate]            │
    └───────────────────────────────────┘
```

**تربیتی ڈیٹا:**

- Robot manipulation کاموں کی 130,000 episodes
- کام: "X اٹھائیں"، "X کو Y میں منتقل کریں"، "دراز کھولیں"، "ڈھکن بند کریں"
- Human teleoperation استعمال کر کے جمع کیا گیا (joystick control)

**Performance:**

- تربیت یافتہ کاموں پر 97% کامیابی
- نئی اشیاء پر 76% کامیابی (تربیت کے دوران نہیں دیکھیں)
- واحد GPU پر 3 Hz (333 ms فی action) پر چلتا ہے

### 2. RT-2 (Robotics Transformer 2)

**Google Research کی طرف سے جاری (2023)**

RT-2 **vision-language models (VLMs)** جیسے PaLM-E اور CLIP استعمال کر کے RT-1 کو بڑھاتا ہے، جو **internet-scale** image-text data پر pretrained ہیں۔

**اہم بصیرت:**

Internet data میں implicit physics اور common-sense reasoning شامل ہے:

- "پانی ڈالنا" کی تصاویر مائع کی dynamics سکھاتی ہیں
- "جار کھولیں" captions twist motions سکھاتے ہیں
- "کرسی منتقل کریں" تصاویر بڑی اشیاء کی manipulation سکھاتی ہیں

RT-2 ان VLMs کو robot data پر **fine-tune** کرتا ہے، web knowledge کو robotics میں منتقل کرتا ہے۔

**فن تعمیر:**

```
Pretrained VLM (PaLM-E / CLIP)
    ↓ Robot data پر Fine-tune
RT-2 Model
    ↓ Input: Image + "کپ میں پانی ڈالیں"
    ↓ Output: [Δx, Δy, Δz, gripper_force]
```

**RT-1 پر Performance بہتریاں:**

- **Emergent skills**: robot data میں کبھی نہ دیکھے گئے کام انجام دے سکتا ہے (مثلاً، "کیلے کو Taylor Swift کے پاس منتقل کریں" → کیلے کو Taylor Swift کے poster کی طرف منتقل کرتا ہے)
- **Symbol understanding**: برانڈ کے ناموں سے اشیاء کو پہچانتا ہے (مثلاً، "Coke کا کین اٹھائیں")
- **Zero-shot generalization**: بالکل نئے کاموں پر 62% کامیابی

**مثال:**

**ہدایت:** "سیب کو کتے کی تصویر کی طرف منتقل کریں"

**کیا ہوتا ہے:**

1. RT-2 "سیب" اور "کتے کی تصویر" کو پہچاننے کے لیے CLIP استعمال کرتا ہے
2. سیب کو کتے کی طرف منتقل کرنے کے لیے حرکت کی منصوبہ بندی کرتا ہے
3. Closed-loop control کے ساتھ عمل کو execute کرتا ہے

کوئی واضح object detection یا motion planning code نہیں—یہ سب سیکھا ہوا ہے!

---

## VLA Models کیسے کام کرتے ہیں: قدم بہ قدم

آئیے دیکھیں کہ VLA model ایک command کو کیسے process کرتا ہے۔

### مثال کا کام: "سرخ مگ اٹھائیں"

**مرحلہ 1: Image Encoding**

```python
# Camera images (مختلف زاویوں سے 6 views)
images = [camera1.read(), camera2.read(), ..., camera6.read()]

# EfficientNet استعمال کر کے images encode کریں (ImageNet پر pretrained)
image_encoder = EfficientNet()
visual_features = image_encoder(images)  # Shape: [6, 512]

# Cameras میں pool کریں
visual_tokens = pool(visual_features)  # Shape: [512]
```

**مرحلہ 2: Language Encoding**

```python
# صارف کی ہدایت
instruction = "سرخ مگ اٹھائیں"

# Universal Sentence Encoder کے ساتھ encode کریں
language_encoder = UniversalSentenceEncoder()
language_tokens = language_encoder(instruction)  # Shape: [512]
```

**مرحلہ 3: Transformer Fusion**

```python
# Vision اور language کو concatenate کریں
input_tokens = concat(visual_tokens, language_tokens)  # Shape: [1024]

# Transformer کے ذریعے pass کریں
transformer = TransformerDecoder(layers=8, heads=128)
action_logits = transformer(input_tokens)  # Shape: [11]
```

**مرحلہ 4: Action Decoding**

```python
# Action space: [x, y, z, roll, pitch, yaw, gripper, terminate]
action = decode_action(action_logits)

# مثال کا output:
# [Δx: +0.02m, Δy: -0.01m, Δz: +0.05m,  # مگ کی طرف منتقل ہوں
#  roll: 0°, pitch: 45°, yaw: 0°,       # Gripper کو جھکائیں
#  gripper: CLOSED, terminate: False]   # Gripper بند کریں، جاری رکھیں
```

**مرحلہ 5: Robot Execution**

```python
# ROS 2 کو action بھیجیں
joint_state_msg = compute_inverse_kinematics(action)
joint_pub.publish(joint_state_msg)
```

**مرحلہ 6: دہرائیں**

Model 3 Hz پر چلتا ہے، کام مکمل ہونے تک نئے camera frames کی بنیاد پر مسلسل adjust کرتا ہے۔

---

## VLA Models کی تربیت: Imitation Learning

VLA models **imitation learning** استعمال کر کے تربیت یافتہ ہیں: ماہر demonstrations سے سیکھنا۔

### تربیتی عمل

**مرحلہ 1: Demonstrations جمع کریں**

انسان robot کو teleoperate کرتے ہیں کہ کام مکمل کریں جبکہ record کرتے ہیں:

- ہر timestep پر camera images
- Language instructions (مثلاً، "سرخ بلاک اٹھائیں")
- Robot actions (joint positions، gripper state)

**مرحلہ 2: Dataset بنائیں**

```python
# مثال کا dataset entry
{
    "instruction": "سرخ بلاک اٹھائیں",
    "images": [img_t0, img_t1, ..., img_t100],  # 100 frames @ 3 Hz = 33 سیکنڈ
    "actions": [a_t0, a_t1, ..., a_t100],       # متعلقہ actions
}
```

**مرحلہ 3: Behavioral Cloning کے ساتھ تربیت دیں**

```python
# Loss function: ماہر actions کی پیش گوئی کریں
loss = MSE(predicted_actions, expert_actions)

# تربیتی loop
for batch in dataset:
    images, instructions, expert_actions = batch
    predicted_actions = vla_model(images, instructions)
    loss = criterion(predicted_actions, expert_actions)
    loss.backward()
    optimizer.step()
```

**مرحلہ 4: Robot پر Fine-Tune کریں**

Model کو robot پر تعینات کریں اور **on-policy** data جمع کریں (سیکھی ہوئی policy سے data، انسانوں سے نہیں)۔ یہ distribution shift کو درست کرتا ہے۔

```python
# DAgger (Dataset Aggregation) algorithm
1. موجودہ policy π تعینات کریں
2. جب policy ناکام ہو، انسان action کو درست کرتا ہے
3. (state, corrected_action) کو dataset میں شامل کریں
4. Policy کو دوبارہ تربیت دیں
5. دہرائیں
```

### VLA تربیت کے لیے Datasets

| Dataset | سائز | کام | ذریعہ |
|---------|------|-------|--------|
| **RT-1 Dataset** | 130K demos | Pick-and-place، drawer opening | Google Robot Kitchens |
| **Open-X Embodiment** | 1M+ demos | 160+ کام، 22 robot types | Multi-institution collaboration |
| **CALVIN** | 24K demos | Long-horizon کام (اوسط 34 steps) | Simulation (table-top) |
| **RoboTurk** | 100K demos | Manipulation کام | Simulated + Real |

**Open-X Embodiment** (2023) سب سے بڑا ہے: 22 مختلف robot platforms سے data کو یکجا کرتا ہے، **cross-embodiment learning** کو ممکن بناتا ہے (ایک robot پر تربیت دیں، دوسرے میں منتقل کریں)۔

---

## VLA کو ROS 2 سے جوڑنا

VLA models عام طور پر Python (PyTorch/TensorFlow) میں تربیت یافتہ ہوتے ہیں۔ یہاں انہیں ROS 2 system میں integrate کرنے کا طریقہ ہے۔

### مثال: Robot پر RT-2 تعینات کرنا

**فن تعمیر:**

```
ROS 2 Node: VLA Control
    ↓ Subscribe: /camera/image, /speech/transcript
    ↓ Publish: /joint_commands
    ↓ استعمال: RT-2 PyTorch model
```

**Implementation:**

```python
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, JointState
from std_msgs.msg import String
import torch
from rt2_model import RT2  # فرضی RT-2 implementation

class VLAControlNode(Node):
    def __init__(self):
        super().__init__('vla_control_node')

        # RT-2 model لوڈ کریں
        self.model = RT2.from_pretrained('google/rt-2-base')
        self.model.eval()  # Inference mode
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)

        # Camera اور language input کو subscribe کریں
        self.create_subscription(Image, '/camera/image', self.image_callback, 10)
        self.create_subscription(String, '/speech/transcript', self.command_callback, 10)

        # Robot actions کے لیے publisher
        self.action_pub = self.create_publisher(JointState, '/joint_commands', 10)

        # State
        self.current_image = None
        self.current_instruction = None

        # 3 Hz پر control loop (RT-2 inference rate)
        self.create_timer(1.0 / 3.0, self.control_loop)

        self.get_logger().info('VLA Control Node ready')

    def image_callback(self, msg):
        # ROS Image کو numpy array میں convert کریں
        self.current_image = self.ros_img_to_numpy(msg)

    def command_callback(self, msg):
        self.current_instruction = msg.data
        self.get_logger().info(f'نئی ہدایت: {self.current_instruction}')

    def control_loop(self):
        if self.current_image is None or self.current_instruction is None:
            return  # دونوں inputs کا انتظار کریں

        # Inputs تیار کریں
        image_tensor = self.preprocess_image(self.current_image)
        instruction_tensor = self.tokenize_instruction(self.current_instruction)

        # VLA model چلائیں
        with torch.no_grad():
            action = self.model(image_tensor, instruction_tensor)

        # Action کو JointState message میں convert کریں
        joint_msg = self.action_to_joint_state(action)
        self.action_pub.publish(joint_msg)

        self.get_logger().info(f'Action شائع کیا: {action.tolist()}')

    def preprocess_image(self, img):
        # 224x224 میں resize کریں، normalize کریں، tensor میں convert کریں
        img_resized = cv2.resize(img, (224, 224))
        img_tensor = torch.from_numpy(img_resized).permute(2, 0, 1).float() / 255.0
        img_tensor = img_tensor.unsqueeze(0).to(self.device)
        return img_tensor

    def tokenize_instruction(self, text):
        # Model کے tokenizer استعمال کریں (مثلاً، SentencePiece)
        tokens = self.model.tokenizer(text, return_tensors='pt').to(self.device)
        return tokens

    def action_to_joint_state(self, action):
        # VLA action [Δx, Δy, Δz, ...] کو joint positions میں convert کریں
        # اس کے لیے inverse kinematics (IK) کی ضرورت ہے
        joint_positions = self.compute_ik(action)

        joint_msg = JointState()
        joint_msg.name = ['joint1', 'joint2', 'joint3', 'joint4', 'joint5', 'joint6', 'gripper']
        joint_msg.position = joint_positions.tolist()

        return joint_msg

    def compute_ik(self, action):
        # Placeholder: IK کے لیے PyBullet یا MoveIt جیسی library استعمال کریں
        # action format: [Δx, Δy, Δz, roll, pitch, yaw, gripper_state]
        # Output: joint angles
        return torch.rand(7)  # Dummy values

    def ros_img_to_numpy(self, msg):
        # ROS Image کو OpenCV format میں convert کریں
        import cv_bridge
        bridge = cv_bridge.CvBridge()
        return bridge.imgmsg_to_cv2(msg, desired_encoding='rgb8')

def main(args=None):
    rclpy.init(args=args)
    node = VLAControlNode()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

**یہ Node کیا کرتا ہے:**

1. Camera images اور voice commands کو subscribe کرتا ہے
2. Actions کی پیش گوئی کرنے کے لیے 3 Hz پر RT-2 model چلاتا ہے
3. Inverse kinematics استعمال کر کے actions کو joint commands میں convert کرتا ہے
4. Robot execution کے لیے `/joint_commands` میں شائع کرتا ہے

---

## آواز → بصارت → عمل: مکمل مثال

آئیے ایک مکمل pipeline بنائیں جہاں صارف robot کو بولی ہوئی commands دے سکتا ہے۔

### System Architecture

```
┌─────────────┐
│  Microphone │ → /audio/raw
└──────┬──────┘
       ↓
┌──────────────┐
│ Whisper Node │ → /speech/transcript
└──────┬───────┘
       ↓
┌──────────────────┐     /camera/image
│  VLA Node (RT-2) │ ← ──────────────
└────────┬─────────┘
         ↓
   /joint_commands
         ↓
┌─────────────────┐
│  Robot Hardware │
└─────────────────┘
```

### مرحلہ 1: Whisper کے ساتھ Speech-to-Text

```python
import rclpy
from rclpy.node import Node
from audio_msgs.msg import AudioData
from std_msgs.msg import String
import whisper

class WhisperNode(Node):
    def __init__(self):
        super().__init__('whisper_node')

        # Whisper model لوڈ کریں
        self.model = whisper.load_model('base')  # 'tiny', 'base', 'small', 'medium', 'large'

        # Audio کو subscribe کریں
        self.create_subscription(AudioData, '/audio/raw', self.audio_callback, 10)

        # Transcripts کے لیے publisher
        self.transcript_pub = self.create_publisher(String, '/speech/transcript', 10)

        self.get_logger().info('Whisper Node تیار ہے')

    def audio_callback(self, msg):
        # Audio کو numpy array میں convert کریں
        audio_array = np.frombuffer(msg.data, dtype=np.float32)

        # Whisper کے ساتھ transcribe کریں
        result = self.model.transcribe(audio_array)
        transcript = result['text']

        self.get_logger().info(f'Transcript: {transcript}')

        # Transcript شائع کریں
        transcript_msg = String()
        transcript_msg.data = transcript
        self.transcript_pub.publish(transcript_msg)
```

### مرحلہ 2: مکمل System لانچ کریں

```bash
# Terminal 1: Camera node لانچ کریں
ros2 run usb_cam usb_cam_node

# Terminal 2: Whisper node لانچ کریں
ros2 run vla_control whisper_node

# Terminal 3: VLA control node لانچ کریں
ros2 run vla_control vla_control_node

# Terminal 4: Robot hardware interface لانچ کریں
ros2 launch my_robot robot.launch.py
```

### مرحلہ 3: اس کی جانچ کریں

```bash
# صارف بولتا ہے: "سرخ کپ اٹھائیں"
# → Whisper "/speech/transcript" میں transcribe کرتا ہے
# → VLA node image + transcript وصول کرتا ہے
# → RT-2 action کی پیش گوئی کرتا ہے
# → Robot حرکت execute کرتا ہے
```

---

## Capstone: خودکار Humanoid Assistant بنانا

آئیے ایک مکمل خودکار humanoid ڈیزائن کریں جو گھریلو کام انجام دے سکتا ہے۔

### System کی ضروریات

**صلاحیتیں:**

1. آواز کے حکم کی تفہیم
2. اشیاء کا پتہ لگانا اور tracking
3. اندرونی ماحول میں navigation
4. Manipulation (اٹھانا، رکھنا، دروازے کھولنا)
5. انسان-روبوٹ تعامل (اشارے، چہرے کے تاثرات)

**Hardware:**

- **Humanoid Platform**: Figure 02، Tesla Optimus، یا Boston Dynamics Atlas
- **Sensors**: RGB-D cameras (2)، LiDAR، IMU، microphones (4)
- **Compute**: NVIDIA Jetson AGX Orin (64 GB RAM، 275 TOPS)
- **Actuators**: 25 DOF (بازو: 7 ہر ایک، ٹانگیں: 6 ہر ایک، torso: 3، سر: 2)

### Software Architecture

```
┌──────────────────────────────────────────────────┐
│            High-Level Planning (GPT-4)           │
│  کام: "میرے لیے سینڈوچ بنائیں"               │
│  → ذیلی کام: [باورچی خانے میں navigate کریں، │
│               fridge کھولیں، روٹی پکڑیں،       │
│               پنیر پکڑیں، ...]                 │
└───────────────┬──────────────────────────────────┘
                ↓
┌──────────────────────────────────────────────────┐
│         VLA Model (RT-2 / Custom)                │
│  ذیلی کام: "روٹی پکڑیں"                       │
│  Input: Camera + "روٹی پکڑیں"                 │
│  Output: End-effector trajectory                │
└───────────────┬──────────────────────────────────┘
                ↓
┌──────────────────────────────────────────────────┐
│          Isaac ROS (Perception)                  │
│  - Object detection (روٹی، پنیر، وغیرہ)       │
│  - Visual SLAM (localization)                   │
│  - Depth estimation (obstacle avoidance)        │
└───────────────┬──────────────────────────────────┘
                ↓
┌──────────────────────────────────────────────────┐
│          ROS 2 Control (MoveIt 2)                │
│  - Path planning (collision-free)               │
│  - Inverse kinematics                           │
│  - Joint trajectory execution                   │
└───────────────┬──────────────────────────────────┘
                ↓
┌──────────────────────────────────────────────────┐
│          Robot Hardware                          │
│  - Actuators (motors)                           │
│  - Sensors (encoders, force/torque)             │
└──────────────────────────────────────────────────┘
```

### Implementation Flow

**مرحلہ 1: GPT-4 کے ساتھ High-Level Planning**

```python
import openai

def plan_task(task):
    prompt = f"""
    آپ ایک robot task planner ہیں۔ اس کام کو ذیلی کاموں میں تقسیم کریں:
    کام: {task}

    Output format (JSON):
    {{"subtasks": ["subtask1", "subtask2", ...]}}
    """

    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )

    plan = json.loads(response.choices[0].message.content)
    return plan['subtasks']

# مثال
task = "میرے لیے سینڈوچ بنائیں"
subtasks = plan_task(task)
# Output: ["باورچی خانے میں navigate کریں", "Fridge کھولیں", "روٹی پکڑیں", "پنیر پکڑیں", "سینڈوچ جوڑیں"]
```

**مرحلہ 2: VLA کے ساتھ ہر ذیلی کام Execute کریں**

```python
for subtask in subtasks:
    # Subtask کو instruction کے طور پر شائع کریں
    instruction_msg = String()
    instruction_msg.data = subtask
    instruction_pub.publish(instruction_msg)

    # VLA node اسے اٹھاتا ہے اور execute کرتا ہے
    # تکمیل کے signal کا انتظار کریں
    wait_for_completion()
```

**مرحلہ 3: VLA Execution (RT-2)**

VLA node (پہلے سے) ہر subtask وصول کرتا ہے اور اسے execute کرتا ہے:

```python
# VLA Node وصول کرتا ہے: "روٹی پکڑیں"
# → Camera کے ذریعے environment دیکھتا ہے
# → روٹی کی طرف منتقل ہونے کے لیے action کی پیش گوئی کرتا ہے
# → Joint commands شائع کرتا ہے
# → Robot روٹی پکڑتا ہے
```

**مرحلہ 4: Isaac ROS کے ساتھ Perception**

```python
# Isaac ROS node scene میں "روٹی" کا پتہ لگاتا ہے
# Object location کو /detected_objects میں شائع کرتا ہے
# VLA node actions کو بہتر بنانے کے لیے اسے استعمال کرتا ہے
```

**مرحلہ 5: MoveIt 2 کے ساتھ Motion Planning**

```python
# MoveIt 2 collision-free trajectory کی منصوبہ بندی کرتا ہے
# /joint_trajectory_controller میں شائع کرتا ہے
# Robot حرکت کو smoothly execute کرتا ہے
```

### حفاظت اور انسانی تعامل

**اہم حفاظتی خصوصیات:**

1. **Force/Torque Limits**: اگر قوت 50N سے زیادہ ہو تو Motors پاور کاٹ دیتے ہیں
2. **Emergency Stop**: Wireless e-stop button تمام حرکت کو روک دیتا ہے
3. **Vision-Based Monitoring**: کام کی جگہ میں انسانوں کا پتہ لگاتا ہے اور سست ہو جاتا ہے
4. **Compliance Control**: غیر متوقع رابطے پر yield ہونے والے نرم joints

**انسان-روبوٹ تعامل:**

```python
# انسانی اشاروں کا پتہ لگائیں (مثلاً، "رکیں" ہاتھ کا نشان)
gesture = detect_gesture(camera_image)

if gesture == "stop":
    robot.stop_all_motion()
    speak("فوری طور پر رک رہا ہوں")

# Feedback کے لیے چہرے کے تاثرات
if task_success:
    robot.display_emotion("خوش")
else:
    robot.display_emotion("confused")
    speak("مجھے یقین نہیں کہ کیا کرنا ہے۔ کیا آپ مدد کر سکتے ہیں؟")
```

---

## اہم نکات

1. **VLA models** vision، language، اور action کو ایک واحد end-to-end learnable system میں متحد کرتے ہیں
2. **RT-1** نے 130K حقیقی دنیا کے robot demos پر imitation learning کا مظاہرہ کیا
3. **RT-2** emergent generalization کے لیے internet-scale vision-language models سے فائدہ اٹھاتا ہے
4. VLA models control nodes کے طور پر **ROS 2** کے ساتھ بغیر کسی رکاوٹ کے integrate ہوتے ہیں
5. **آواز → بصارت → عمل** pipelines قدرتی انسان-روبوٹ تعامل کو ممکن بناتے ہیں
6. ایک مکمل خودکار humanoid **VLA**، **Isaac ROS**، **MoveIt 2**، اور **GPT-4** کو یکجا کرتا ہے

---

## ہاتھ سے عملی مشق

### ورزش 1: VLA Inference کی رفتار کا حساب لگائیں

اگر RT-2 3 Hz (333 ms فی action) پر چلتا ہے، اور ایک کام کو مکمل کرنے کے لیے 100 actions کی ضرورت ہے:

- کام کو کتنا وقت لگتا ہے؟
- اگر آپ 2x تیز GPU میں upgrade کرتے ہیں، تو نیا کام مکمل ہونے کا وقت کیا ہے؟

### ورزش 2: VLA تربیتی Dataset ڈیزائن کریں

آپ کپڑے تہہ کرنے کے لیے VLA model کی تربیت دینا چاہتے ہیں۔ آپ کون سے demonstrations جمع کریں گے؟

**اشارہ:** تبدیلیوں کے بارے میں سوچیں (قمیض کے سائز، fabric کی اقسام، ابتدائی positions)۔

### ورزش 3: Hybrid Planning

"میرے لیے سینڈوچ بنائیں" کام کے لیے، کون سے ذیلی کاموں کو استعمال کرنا چاہیے:

- **GPT-4 planning** (high-level)
- **VLA execution** (low-level)
- **Classical control** (precise motions)

**مثال کا جواب:**

- GPT-4: "باورچی خانے میں navigate کریں" (high-level)
- VLA: "روٹی پکڑیں" (manipulation)
- Classical: "45° زاویے پر مکھن پھیلائیں" (precise force control)

### ورزش 4: حفاظتی ناکامی کے طریقے

ایک humanoid ریستوران میں مشروبات پیش کر رہا ہے۔ 3 ناکامی کے طریقے اور تخفیف کی حکمت عملیاں بتائیں۔

**اشارہ:** بہاؤ، تصادم، اور غلط فہمیوں پر غور کریں۔

---

## عام غلطیاں جو ابتدائی کرتے ہیں

1. **صرف کامیاب demos پر تربیت دینا**: غلطی کی بازیابی سکھانے کے لیے ناکامی کے معاملات شامل کریں
2. **Action frequency کو نظر انداز کرنا**: VLA models مستقل control rates (مثلاً، 3 Hz) کی توقع کرتے ہیں
3. **Demo environments میں Overfitting**: domain randomization استعمال کریں (lighting، backgrounds)
4. **انسانی feedback چھوڑنا**: اصلاحی demonstrations (DAgger) کے ساتھ fine-tune کریں
5. **جلد حقیقی hardware پر testing نہیں کرنا**: VLAs کے لیے بھی simulation-to-real gap موجود ہے

---

## آگے کیا ہے؟

اگلے باب میں، ہم:

- شروع سے custom VLA model کی تربیت دیں گے
- Teleoperation استعمال کر کے demonstrations جمع کریں گے
- نئے کام پر RT-2 کو fine-tune کریں گے
- ROS 2 کے ساتھ حقیقی robot پر model تعینات کریں گے

اب آپ نے VLA paradigm میں مہارت حاصل کر لی ہے۔ اگلا، ہم production-ready system بنائیں گے!

---

**گہری سیکھنے کے لیے وسائل:**

- [RT-1 Paper](https://robotics-transformer.github.io/)
- [RT-2 Paper](https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/)
- [Open-X Embodiment Dataset](https://robotics-transformer-x.github.io/)
- [Whisper (OpenAI Speech Recognition)](https://github.com/openai/whisper)
- [ROS 2 Control Tutorials](https://control.ros.org/master/index.html)
