---
id: chapter-1-basics
title: "Chapter 1 - ROS 2 Basics"
sidebar_label: "Chapter 1: Basics"
---

import ChapterActionsBar from '@site/src/components/learning/ChapterActionsBar';

# Chapter 1: ROS 2 Basics

<ChapterActionsBar />

## Introduction to Physical AI and Embodied Intelligence

Before diving into ROS 2, let's understand what Physical AI means and why it's the next frontier in artificial intelligence.

### What is Physical AI?

**Physical AI** refers to AI systems that interact with and operate in the real physical world. Unlike traditional AI that exists only in digital spaces (like chatbots or recommendation systems), Physical AI must:

- Understand physical laws (gravity, friction, inertia)
- Process sensory data from cameras, LiDAR, and touch sensors
- Execute actions that affect the real world through motors and actuators
- Adapt to unpredictable, dynamic environments

Think of it this way: ChatGPT is brilliant at conversation, but it can't pick up a cup of coffee. A humanoid robot with Physical AI can do both—understand your request *and* physically fulfill it.

### Embodied Intelligence: AI That Lives in a Body

**Embodied intelligence** is the idea that intelligence emerges from the interaction between a body and its environment. A brain without sensors and actuators can't learn about the world through experience.

Humanoid robots excel in our world because:

1. **Form Factor Match**: They navigate spaces designed for humans (stairs, doorways, chairs)
2. **Tool Compatibility**: They use human tools without redesign
3. **Social Intuition**: People naturally understand their body language
4. **Rich Training Data**: Human motion data is abundant (videos, motion capture)

This is why companies like Tesla (Optimus), Figure AI, and Boston Dynamics are investing billions in humanoid robotics.

---

## Why ROS 2? The Robotic Nervous System

Imagine building a robot from scratch. You'd need to:

- Read data from 20+ sensors simultaneously
- Send commands to dozens of motors
- Run AI models for vision, speech, and planning
- Coordinate all these tasks in real-time
- Debug when something goes wrong

This is where **ROS 2 (Robot Operating System 2)** comes in. It's not an operating system like Windows or Linux—it's a **middleware framework** that provides:

- **Communication infrastructure** between different parts of your robot
- **Standard tools** for visualization, debugging, and recording data
- **A massive ecosystem** of pre-built libraries for navigation, perception, and control

Think of ROS 2 as the nervous system that connects the robot's "brain" (AI), "senses" (sensors), and "muscles" (actuators).

### ROS 1 vs ROS 2: Why the Upgrade?

ROS 1 was revolutionary but had limitations:

- No built-in real-time support (critical for safety)
- Poor Windows compatibility
- Limited security features
- Single-point-of-failure architecture

**ROS 2** (released 2017, stable since 2020) addresses all these issues with:

- **DDS (Data Distribution Service)**: Industry-standard real-time communication
- **Multi-platform support**: Linux, Windows, macOS
- **Security**: Encryption and authentication built-in
- **Scalability**: Works from tiny drones to warehouse robot fleets

For Physical AI, ROS 2 is the standard choice.

---

## Core ROS 2 Concepts

ROS 2 organizes robot software into a **graph of independent programs** that communicate with each other. Let's break down the key concepts.

### 1. Nodes: The Building Blocks

A **node** is a single program that performs one specific task. Examples:

- A node that reads camera images
- A node that detects faces in images
- A node that controls the robot's arm

**Why separate nodes instead of one big program?**

- **Modularity**: Replace the face detector without touching the arm controller
- **Debugging**: Test each part independently
- **Parallelism**: Different nodes run on different CPU cores
- **Reusability**: Share nodes across different robots

**Real-World Analogy**: Think of a restaurant kitchen. The chef (one node) doesn't also wash dishes and take orders. Specialized staff (nodes) handle each task and communicate through order tickets (messages).

### 2. Topics: The Data Highways

**Topics** are named channels over which nodes send and receive messages.

- A node **publishes** messages to a topic
- Other nodes **subscribe** to that topic to receive the messages

```
┌──────────────┐         /camera/image         ┌──────────────┐
│ Camera Node  │────────────────────────────▶│ Vision Node  │
│ (Publisher)  │                               │ (Subscriber) │
└──────────────┘                               └──────────────┘
```

**Key Properties:**

- **Many-to-many**: Multiple publishers and subscribers on one topic
- **Asynchronous**: Publisher doesn't wait for subscribers
- **Typed**: Each topic has a specific message type (like `Image`, `LaserScan`)

**Real-World Analogy**: Topics are like radio stations. A news station (publisher) broadcasts continuously. Anyone with a radio (subscriber) tuned to that frequency receives the news. The station doesn't know or care how many listeners there are.

### 3. Services: Request-Response Communication

Sometimes you need a **synchronous request-response** pattern. That's where **services** come in.

```
┌──────────────┐     Request: "Add 2+3"      ┌──────────────┐
│ Client Node  │──────────────────────────▶│ Server Node  │
│              │◀──────────────────────────│ (Calculator) │
└──────────────┘     Response: "5"          └──────────────┘
```

**When to use services:**

- Configuration changes (e.g., "Set camera exposure to 100")
- One-time queries (e.g., "What's the robot's battery level?")
- Actions that must complete before proceeding

**Difference from Topics:**

| Feature | Topics | Services |
|---------|--------|----------|
| Pattern | Publish-Subscribe | Request-Response |
| Timing | Asynchronous (continuous) | Synchronous (one-shot) |
| Use Case | Sensor data streams | Configuration, queries |

### 4. Actions: Long-Running Tasks

**Actions** are like services but for tasks that take time and provide feedback.

```
Goal: "Navigate to kitchen"
    ↓
Feedback: "10% complete... 50% complete..."
    ↓
Result: "Arrived at kitchen"
```

**Examples:**

- Navigate to a location (with progress updates)
- Pick up an object (with approach/grasp/lift phases)
- Charge battery (with charging status updates)

Actions can be **canceled** mid-execution, unlike services.

---

## ROS 2 in Python: Your First Node

Let's write a simple ROS 2 node in Python using `rclpy` (ROS Client Library for Python).

### Example 1: Hello World Publisher

This node publishes a "Hello, World!" message every second.

```python
import rclpy
from rclpy.node import Node
from std_msgs.msg import String

class HelloPublisher(Node):
    def __init__(self):
        super().__init__('hello_publisher')  # Node name

        # Create a publisher for the 'greetings' topic
        self.publisher = self.create_publisher(String, 'greetings', 10)

        # Create a timer that calls publish_message() every 1.0 seconds
        self.timer = self.create_timer(1.0, self.publish_message)

        self.count = 0

    def publish_message(self):
        msg = String()
        msg.data = f'Hello, World! Count: {self.count}'
        self.publisher.publish(msg)
        self.get_logger().info(f'Published: "{msg.data}"')
        self.count += 1

def main(args=None):
    rclpy.init(args=args)              # Initialize ROS 2
    node = HelloPublisher()            # Create the node
    rclpy.spin(node)                   # Keep the node running
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

**What's happening here?**

1. **Import `rclpy`**: The Python ROS 2 library
2. **Inherit from `Node`**: Every ROS 2 node is a class inheriting from `Node`
3. **Create a publisher**: `create_publisher(MessageType, 'topic_name', queue_size)`
4. **Timer callback**: Calls `publish_message()` repeatedly
5. **Spin**: Keeps the node alive and processing callbacks

### Example 2: Subscriber Node

This node listens to the `greetings` topic and prints received messages.

```python
import rclpy
from rclpy.node import Node
from std_msgs.msg import String

class HelloSubscriber(Node):
    def __init__(self):
        super().__init__('hello_subscriber')

        # Subscribe to the 'greetings' topic
        self.subscription = self.create_subscription(
            String,
            'greetings',
            self.listener_callback,
            10)

    def listener_callback(self, msg):
        self.get_logger().info(f'Received: "{msg.data}"')

def main(args=None):
    rclpy.init(args=args)
    node = HelloSubscriber()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

**How to run:**

```bash
# Terminal 1: Run publisher
python3 hello_publisher.py

# Terminal 2: Run subscriber
python3 hello_subscriber.py
```

You'll see the subscriber printing messages published by the first node!

---

## Understanding URDF: Describing Your Robot

**URDF (Unified Robot Description Format)** is an XML format for describing a robot's physical structure.

### Why URDF?

When you simulate or control a robot, ROS 2 needs to know:

- What parts (links) does the robot have?
- How are they connected (joints)?
- What are their sizes, masses, and collision properties?
- Where are the sensors attached?

URDF provides this "blueprint."

### Simple URDF Example: A Two-Link Arm

```xml
<?xml version="1.0"?>
<robot name="simple_arm">

  <!-- Base Link (fixed to ground) -->
  <link name="base_link">
    <visual>
      <geometry>
        <cylinder length="0.1" radius="0.05"/>
      </geometry>
      <material name="blue">
        <color rgba="0 0 1 1"/>
      </material>
    </visual>
  </link>

  <!-- Shoulder Joint (revolute = rotates) -->
  <joint name="shoulder_joint" type="revolute">
    <parent link="base_link"/>
    <child link="upper_arm"/>
    <origin xyz="0 0 0.1" rpy="0 0 0"/>
    <axis xyz="0 1 0"/>  <!-- Rotates around Y-axis -->
    <limit lower="-1.57" upper="1.57" effort="10" velocity="1"/>
  </joint>

  <!-- Upper Arm Link -->
  <link name="upper_arm">
    <visual>
      <geometry>
        <box size="0.05 0.05 0.3"/>
      </geometry>
      <material name="red">
        <color rgba="1 0 0 1"/>
      </material>
    </visual>
  </link>

</robot>
```

**Key Elements:**

- **`<link>`**: A rigid body (e.g., arm segment, wheel)
- **`<joint>`**: Connection between two links
  - `type="revolute"`: Rotates (like elbow)
  - `type="prismatic"`: Slides (like telescope)
  - `type="fixed"`: No movement
- **`<origin>`**: Position/orientation in 3D space
- **`<geometry>`**: Shape for visualization/collision

For a humanoid robot, you'd have:

- ~30-40 links (head, torso, arms, legs, hands)
- ~25-30 joints (shoulders, elbows, knees, ankles)
- Sensor definitions (cameras, IMUs)

---

## Bridging Python AI Agents to ROS 2

One of the most powerful aspects of ROS 2 is connecting AI models (running in Python) to robot hardware.

### Scenario: Voice Command to Robot Motion

Let's say you want a robot to respond to "Wave hello."

**Pipeline:**

1. **Speech-to-Text**: OpenAI Whisper (Python)
2. **Intent Recognition**: GPT-4 determines "wave" action (Python)
3. **Motion Planning**: ROS 2 MoveIt plans arm trajectory
4. **Execution**: Arm controller executes motion

Here's a simplified ROS 2 node that bridges AI to action:

```python
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from sensor_msgs.msg import JointState
import openai  # Example AI library

class VoiceCommandNode(Node):
    def __init__(self):
        super().__init__('voice_command_node')

        # Subscribe to speech-to-text results
        self.create_subscription(String, '/voice/transcript', self.on_voice_command, 10)

        # Publish joint commands to the robot
        self.joint_pub = self.create_publisher(JointState, '/joint_commands', 10)

        self.get_logger().info('Voice command node ready')

    def on_voice_command(self, msg):
        transcript = msg.data
        self.get_logger().info(f'Heard: {transcript}')

        # Use AI to interpret command
        action = self.interpret_command(transcript)

        if action == 'wave':
            self.execute_wave()

    def interpret_command(self, text):
        # Simplified: In reality, use GPT-4 API
        if 'wave' in text.lower():
            return 'wave'
        return 'unknown'

    def execute_wave(self):
        # Create a joint command message
        joint_msg = JointState()
        joint_msg.name = ['shoulder_joint', 'elbow_joint']
        joint_msg.position = [1.0, 0.5]  # Target angles in radians

        self.joint_pub.publish(joint_msg)
        self.get_logger().info('Executing wave motion')

def main(args=None):
    rclpy.init(args=args)
    node = VoiceCommandNode()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

**What this demonstrates:**

- AI processing (speech/language) happens in Python
- Results are published to ROS 2 topics
- Motion controllers (could be in C++ for speed) subscribe and act
- **Separation of concerns**: AI experts work on models, robotics engineers work on control

---

## Visualizing the ROS 2 Graph

ROS 2 provides powerful tools to understand your system.

### Command: `ros2 node list`

Shows all active nodes:

```
/hello_publisher
/hello_subscriber
/voice_command_node
```

### Command: `ros2 topic list`

Shows all topics:

```
/greetings
/voice/transcript
/joint_commands
```

### Command: `ros2 topic echo /greetings`

Prints messages on a topic in real-time:

```
data: 'Hello, World! Count: 0'
---
data: 'Hello, World! Count: 1'
---
```

### Tool: `rqt_graph`

A graphical tool that shows nodes and topics as a visual graph:

```
┌─────────────┐      /greetings      ┌─────────────┐
│  publisher  │────────────────────▶│ subscriber  │
└─────────────┘                      └─────────────┘
```

This is invaluable for debugging complex systems.

---

## Key Takeaways

1. **Physical AI** means AI that understands and acts in the real world through embodied intelligence
2. **ROS 2** is the industry-standard middleware for connecting robot components
3. **Nodes** are independent programs; **topics** are communication channels
4. **Services** handle request-response; **actions** handle long-running tasks with feedback
5. **URDF** describes robot structure for simulation and control
6. **Python + ROS 2** (`rclpy`) lets you bridge AI models to robot hardware seamlessly

---

## Hands-On Practice

To solidify these concepts, try the following:

### Exercise 1: Modify the Hello World Publisher

Change the publisher to send the current timestamp instead of a counter.

**Hint:** Use Python's `time` module.

### Exercise 2: Create a Service

Write a ROS 2 service that:
- Takes two numbers as a request
- Returns their sum as a response

**Hint:** Look up `example_interfaces/srv/AddTwoInts` message type.

### Exercise 3: URDF Exploration

Draw a simple robot arm on paper with 2 joints. Then, try to describe it in URDF format with:
- 3 links (base, upper arm, lower arm)
- 2 revolute joints

You don't need to run this yet—just practice the syntax.

### Exercise 4: Conceptual Bridge

Imagine you have a camera node publishing images on `/camera/image`. You want a face detection node to process those images and publish face locations on `/faces/detected`.

**Question:** What would be the topic subscription and publication in the face detection node?

---

## Common Mistakes Beginners Make

1. **Confusing ROS 2 with an OS**: ROS 2 runs *on top of* Linux/Windows, not instead of it
2. **Forgetting `rclpy.spin()`**: Without spin, callbacks never execute
3. **Topic name typos**: `/camera/image` vs `/camera/images`—must match exactly
4. **Not checking message types**: Publishing `String` to a topic expecting `Int32` fails silently
5. **Over-engineering early**: Start with simple pub-sub before jumping to complex actions

---

## What's Next?

In the next chapter, we'll:

- Set up a ROS 2 workspace
- Build a custom package
- Create a launch file to start multiple nodes at once
- Simulate a simple robot in Gazebo

You've now mastered the conceptual foundation. Next, we get hands-on!

---

**Resources for Deeper Learning:**

- [Official ROS 2 Documentation](https://docs.ros.org/en/humble/)
- [ROS 2 Tutorials](https://docs.ros.org/en/humble/Tutorials.html)
- [URDF XML Specification](http://wiki.ros.org/urdf/XML)
- [rclpy API Reference](https://docs.ros2.org/latest/api/rclpy/)
