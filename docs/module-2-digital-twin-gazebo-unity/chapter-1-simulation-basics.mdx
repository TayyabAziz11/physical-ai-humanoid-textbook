---
id: chapter-1-simulation-basics
title: "Chapter 1 - Simulation Basics"
sidebar_label: "Chapter 1: Simulation Basics"
---

import ChapterActionsBar from '@site/src/components/learning/ChapterActionsBar';

# Chapter 1: Simulation Basics

<ChapterActionsBar />

## What is a Digital Twin?

A **Digital Twin** is a virtual replica of a physical system that mirrors its behavior in real-time or simulated conditions. In robotics, a digital twin lets you:

- **Test before building**: Validate robot designs without expensive hardware
- **Train safely**: Teach robots dangerous tasks (like handling fire) in simulation
- **Generate data**: Create millions of training examples for AI models
- **Debug faster**: Replay and slow down scenarios that fail in the real world

Think of it like a flight simulator for pilots. Before flying a real 747, pilots train thousands of hours in simulators. Similarly, robots can train millions of times in simulation before touching the physical world.

### Why Digital Twins Matter for Physical AI

Training robots in the real world is:

- **Expensive**: Breaking a $50,000 humanoid robot during training is costly
- **Slow**: Real-world data collection takes months
- **Dangerous**: Testing navigation near stairs or traffic risks damage
- **Limited**: You can't simulate rain, fire, or zero-gravity in a lab

**Digital twins solve this.** You can:

- Simulate 10 years of experience in 1 day
- Test in environments you don't physically have (Mars, underwater, etc.)
- Parallelize training across hundreds of virtual robots
- Safely fail and learn from mistakes

---

## Why Simulate Before Deploying?

Imagine you're building a humanoid robot to work in a warehouse. Without simulation, you'd need to:

1. Build the full robot (months + $100k+)
2. Set up a test warehouse
3. Manually test every scenario (walking, picking, avoiding obstacles)
4. Fix bugs by modifying hardware (expensive!)
5. Repeat

**With simulation:**

1. Model the robot in software (days)
2. Create virtual warehouse environments
3. Test thousands of scenarios overnight
4. Fix bugs in code (cheap!)
5. Deploy to hardware only when confident

**Result:** Faster development, lower costs, safer testing.

### Real-World Example: Tesla Optimus

Tesla trains its Optimus humanoid robot using simulation:

- They create virtual homes, factories, and warehouses
- The robot practices tasks like folding laundry, picking parts, walking on uneven ground
- AI learns from millions of simulated attempts
- Only validated behaviors are deployed to physical robots

This **sim-to-real transfer** is the key to scalable robot learning.

---

## Physics Simulation: Teaching Robots About Gravity

Robots operate in the physical world, which means they must obey:

- **Gravity**: Objects fall down at 9.8 m/s²
- **Collision**: Two objects can't occupy the same space
- **Friction**: Wheels slip on ice, grip on asphalt
- **Inertia**: Heavy objects resist acceleration

A **physics engine** simulates these laws mathematically so robots learn realistic behavior.

### Key Physics Concepts

#### 1. Gravity

Every object experiences a downward force. In simulation, you set:

```python
gravity = [0, 0, -9.81]  # Standard Earth gravity (Z-axis points up)
```

Without gravity, objects float. Too much, and your robot can't stand.

#### 2. Collision Detection

The simulator checks if objects overlap and prevents penetration.

**Example:** If a robot's hand moves toward a box, collision detection:
- Detects contact
- Applies reaction forces (hand stops, box might move)
- Simulates grip or slip based on friction

#### 3. Friction

Determines how objects slide against each other.

**Low friction** (ice): Robot wheels spin without moving forward
**High friction** (rubber on concrete): Robot has traction

In Gazebo, you define friction per material:

```xml
<friction>
  <ode>
    <mu>0.8</mu>  <!-- Coefficient of friction (0=ice, 1=rubber) -->
  </ode>
</friction>
```

#### 4. Mass and Inertia

Heavier objects are harder to move and rotate.

**Example:** A robot arm carrying a 5kg box requires more torque than lifting an empty hand.

In URDF, you specify:

```xml
<inertial>
  <mass value="5.0"/>  <!-- kg -->
  <inertia ixx="0.1" ixy="0" ixz="0" iyy="0.1" iyz="0" izz="0.1"/>
</inertial>
```

Accurate physics means **sim-to-real transfer works**—what succeeds in simulation succeeds in reality.

---

## Gazebo: The Industry-Standard Robot Simulator

**Gazebo** is an open-source 3D robot simulator tightly integrated with ROS 2. It's the default choice for:

- Academic research (used in thousands of papers)
- Robotics competitions (DARPA Robotics Challenge)
- Industrial prototyping

### Why Gazebo?

- **Free and open-source**
- **Tight ROS 2 integration**: Same ROS 2 code runs in sim and real hardware
- **Realistic physics**: Built on ODE, Bullet, or Simbody engines
- **Sensor simulation**: LiDAR, cameras, IMUs, GPS
- **Extensible**: Write custom plugins in C++/Python

### Gazebo Architecture

Gazebo consists of:

1. **Gazebo Server**: Runs physics simulation (headless, can run on cloud)
2. **Gazebo Client (GUI)**: Visualizes the simulation
3. **Plugins**: Custom behaviors (sensors, controllers, etc.)

```
┌─────────────────────┐
│   Gazebo Client     │ ← Visualizes (optional)
│       (GUI)         │
└──────────┬──────────┘
           │
┌──────────▼──────────┐
│  Gazebo Server      │ ← Runs physics + sensors
│  (Physics Engine)   │
└──────────┬──────────┘
           │
┌──────────▼──────────┐
│   ROS 2 Bridge      │ ← Publishes sensor data to ROS 2 topics
└─────────────────────┘
```

### Installing Gazebo (Ubuntu 22.04)

Gazebo comes in multiple versions. For ROS 2 Humble, use **Gazebo Fortress** or **Gazebo Garden**.

```bash
# Install Gazebo Garden
sudo apt update
sudo apt install gz-garden

# Verify installation
gz sim --version
```

For ROS 2 integration:

```bash
sudo apt install ros-humble-ros-gz
```

---

## Your First Gazebo Simulation

Let's launch a simple world with a ground plane and gravity.

### Step 1: Create a World File

Create `empty_world.sdf`:

```xml
<?xml version="1.0"?>
<sdf version="1.9">
  <world name="empty_world">

    <!-- Physics settings -->
    <physics name="1ms" type="ode">
      <max_step_size>0.001</max_step_size>  <!-- 1ms timestep -->
      <real_time_factor>1</real_time_factor>  <!-- 1x real-time speed -->
    </physics>

    <!-- Lighting -->
    <light name="sun" type="directional">
      <pose>0 0 10 0 0 0</pose>
      <diffuse>1 1 1 1</diffuse>
    </light>

    <!-- Ground plane -->
    <model name="ground_plane">
      <static>true</static>  <!-- Doesn't move -->
      <link name="link">
        <collision name="collision">
          <geometry>
            <plane>
              <normal>0 0 1</normal>  <!-- Points up -->
            </plane>
          </geometry>
        </collision>
        <visual name="visual">
          <geometry>
            <plane>
              <normal>0 0 1</normal>
            </plane>
          </geometry>
          <material>
            <ambient>0.8 0.8 0.8 1</ambient>
          </material>
        </visual>
      </link>
    </model>

  </world>
</sdf>
```

### Step 2: Launch Gazebo

```bash
gz sim empty_world.sdf
```

You'll see a gray ground plane with lighting. This is your blank canvas!

### Step 3: Add a Simple Robot

Let's add a wheeled robot. Create `simple_robot.sdf`:

```xml
<?xml version="1.0"?>
<sdf version="1.9">
  <model name="simple_robot">
    <pose>0 0 0.1 0 0 0</pose>  <!-- Position: X Y Z Roll Pitch Yaw -->

    <!-- Main body -->
    <link name="body">
      <inertial>
        <mass>10.0</mass>
        <inertia>
          <ixx>0.5</ixx>
          <iyy>0.5</iyy>
          <izz>0.5</izz>
        </inertia>
      </inertial>

      <collision name="body_collision">
        <geometry>
          <box>
            <size>0.5 0.3 0.2</size>  <!-- Length Width Height -->
          </box>
        </geometry>
      </collision>

      <visual name="body_visual">
        <geometry>
          <box>
            <size>0.5 0.3 0.2</size>
          </box>
        </geometry>
        <material>
          <ambient>0 0 1 1</ambient>  <!-- Blue -->
        </material>
      </visual>
    </link>

    <!-- Left wheel -->
    <link name="left_wheel">
      <pose>0.15 0.2 0 -1.5707 0 0</pose>  <!-- Rotated 90° to roll -->
      <inertial>
        <mass>1.0</mass>
        <inertia>
          <ixx>0.01</ixx>
          <iyy>0.01</iyy>
          <izz>0.01</izz>
        </inertia>
      </inertial>

      <collision name="collision">
        <geometry>
          <cylinder>
            <radius>0.1</radius>
            <length>0.05</length>
          </cylinder>
        </geometry>
      </collision>

      <visual name="visual">
        <geometry>
          <cylinder>
            <radius>0.1</radius>
            <length>0.05</length>
          </cylinder>
        </geometry>
        <material>
          <ambient>0 1 0 1</ambient>  <!-- Green -->
        </material>
      </visual>
    </link>

    <!-- Joint connecting body to left wheel -->
    <joint name="left_wheel_joint" type="revolute">
      <parent>body</parent>
      <child>left_wheel</child>
      <axis>
        <xyz>0 1 0</xyz>  <!-- Rotates around Y-axis -->
        <limit>
          <lower>-1e16</lower>  <!-- Unlimited rotation -->
          <upper>1e16</upper>
        </limit>
      </axis>
    </joint>

    <!-- Right wheel (mirror of left) -->
    <link name="right_wheel">
      <pose>0.15 -0.2 0 -1.5707 0 0</pose>
      <!-- (Same structure as left_wheel) -->
    </link>

    <joint name="right_wheel_joint" type="revolute">
      <parent>body</parent>
      <child>right_wheel</child>
      <axis>
        <xyz>0 1 0</xyz>
      </axis>
    </joint>

  </model>
</sdf>
```

Include this robot in `empty_world.sdf` by adding:

```xml
<include>
  <uri>model://simple_robot</uri>
</include>
```

Now when you launch Gazebo, you'll see a blue box with two green wheels!

---

## Simulating Sensors: LiDAR, Cameras, and IMUs

Robots perceive the world through sensors. Gazebo simulates these sensors and publishes data to ROS 2 topics.

### 1. LiDAR (Laser Scanner)

**LiDAR** shoots laser beams and measures distance to obstacles. Essential for navigation.

Add to your robot's URDF/SDF:

```xml
<sensor name="lidar" type="gpu_lidar">
  <pose>0.2 0 0.15 0 0 0</pose>  <!-- Mounted on front of robot -->
  <topic>/scan</topic>  <!-- Publishes to this ROS 2 topic -->

  <update_rate>10</update_rate>  <!-- 10Hz -->

  <lidar>
    <scan>
      <horizontal>
        <samples>360</samples>  <!-- 360° coverage -->
        <resolution>1</resolution>
        <min_angle>-3.14159</min_angle>
        <max_angle>3.14159</max_angle>
      </horizontal>
    </scan>
    <range>
      <min>0.1</min>  <!-- Minimum detection distance -->
      <max>10.0</max>  <!-- Maximum detection distance -->
    </range>
  </lidar>
</sensor>
```

**In ROS 2**, subscribe to `/scan` to get distance measurements:

```python
from sensor_msgs.msg import LaserScan

def lidar_callback(msg):
    print(f"Detected obstacle at {min(msg.ranges)} meters")
```

### 2. Depth Camera

**Depth cameras** (like Intel RealSense) provide RGB images + depth information.

```xml
<sensor name="depth_camera" type="depth_camera">
  <pose>0.2 0 0.2 0 0 0</pose>
  <topic>/camera/depth</topic>

  <update_rate>30</update_rate>  <!-- 30 FPS -->

  <camera>
    <horizontal_fov>1.5707</horizontal_fov>  <!-- 90° field of view -->
    <image>
      <width>640</width>
      <height>480</height>
    </image>
    <clip>
      <near>0.1</near>
      <far>10.0</far>
    </clip>
  </camera>
</sensor>
```

**Use case:** Object detection, obstacle avoidance, 3D mapping.

### 3. IMU (Inertial Measurement Unit)

**IMUs** measure acceleration and rotation. Critical for balance in humanoids.

```xml
<sensor name="imu" type="imu">
  <pose>0 0 0.1 0 0 0</pose>
  <topic>/imu</topic>

  <update_rate>100</update_rate>  <!-- 100Hz for fast balance control -->

  <imu>
    <angular_velocity>
      <x><noise type="gaussian"><stddev>0.01</stddev></noise></x>
      <y><noise type="gaussian"><stddev>0.01</stddev></noise></y>
      <z><noise type="gaussian"><stddev>0.01</stddev></noise></z>
    </angular_velocity>
    <linear_acceleration>
      <x><noise type="gaussian"><stddev>0.1</stddev></noise></x>
      <y><noise type="gaussian"><stddev>0.1</stddev></noise></y>
      <z><noise type="gaussian"><stddev>0.1</stddev></noise></z>
    </linear_acceleration>
  </imu>
</sensor>
```

**In ROS 2**:

```python
from sensor_msgs.msg import Imu

def imu_callback(msg):
    print(f"Acceleration: {msg.linear_acceleration}")
    print(f"Angular velocity: {msg.angular_velocity}")
```

---

## Unity for High-Fidelity Rendering

While Gazebo excels at physics, **Unity** provides:

- **Photorealistic rendering**: Lighting, shadows, reflections
- **Human character animation**: For human-robot interaction scenarios
- **VR/AR integration**: Train robots in mixed-reality environments

### Gazebo vs Unity: When to Use Each

| Feature | Gazebo | Unity |
|---------|--------|-------|
| **Physics accuracy** | Excellent | Good |
| **Visual quality** | Basic | Photorealistic |
| **ROS 2 integration** | Native | Requires bridge |
| **Speed** | Fast (headless mode) | Slower (rendering overhead) |
| **Use case** | Navigation, SLAM, control | Computer vision, HRI |

**Common workflow:**

1. Develop and test in **Gazebo** (fast iteration)
2. Final training in **Unity** (realistic visuals for vision models)
3. Deploy to **real hardware**

### Unity-ROS 2 Bridge

Unity doesn't natively speak ROS 2. You need a bridge:

```
Unity Simulator
      ↓
   TCP/IP
      ↓
ROS 2 Bridge Node
      ↓
ROS 2 Topics
```

**Setup (simplified):**

1. Install Unity Robotics Hub: [https://github.com/Unity-Technologies/Unity-Robotics-Hub](https://github.com/Unity-Technologies/Unity-Robotics-Hub)
2. Install ROS 2 connector
3. Configure endpoints

**Example:** Publishing camera images from Unity to ROS 2:

```csharp
// Unity C# script
using Unity.Robotics.ROSTCPConnector;

void Start() {
    ROSConnection.GetOrCreateInstance().Subscribe<ImageMsg>("/camera/image", ProcessImage);
}

void ProcessImage(ImageMsg msg) {
    // Display image in Unity
}
```

---

## Sim-to-Real Transfer: The Challenge

**Sim-to-real transfer** is making robots trained in simulation work in the real world.

### The Reality Gap

Simulations are simplified:

- Physics isn't perfect (friction, deformation)
- Sensors are idealized (no lens distortion, noise)
- Environments are cleaner (no dust, lighting changes)

**Result:** A robot that works perfectly in simulation may fail in reality.

### Solutions

#### 1. Domain Randomization

Vary simulation parameters during training:

- Randomize lighting, textures, object positions
- Add sensor noise
- Vary friction, mass, and gravity slightly

**Benefit:** Robot learns robust policies that handle uncertainty.

#### 2. Accurate Calibration

Measure real robot's parameters:

- Actual link lengths (not CAD model)
- True motor torque curves
- Sensor calibration data

**Benefit:** Simulation matches reality more closely.

#### 3. Incremental Transfer

1. Train basic skills in sim (e.g., standing)
2. Fine-tune on real hardware (e.g., balance on uneven ground)

**Benefit:** Leverage sim speed + real-world accuracy.

---

## Real-World Analogy: Learning to Drive

Think of robot learning like learning to drive:

- **Simulation**: Practice in a driving simulator (safe, cheap, repeatable)
- **Reality gap**: Simulator pedals feel different than a real car
- **Sim-to-real transfer**: First drive in an empty parking lot, then city streets

**The combination** of simulation + real practice is the fastest path to mastery.

---

## Key Takeaways

1. **Digital twins** let robots train safely and quickly in virtual worlds
2. **Physics simulation** (gravity, collision, friction) teaches robots realistic behavior
3. **Gazebo** is the industry standard for ROS 2 robot simulation
4. **Sensors** (LiDAR, depth cameras, IMUs) are simulated to provide realistic data
5. **Unity** complements Gazebo with photorealistic rendering for vision tasks
6. **Sim-to-real transfer** requires domain randomization and calibration to bridge the reality gap

---

## Hands-On Practice

### Exercise 1: Drop Test

In Gazebo, create a cube suspended 1 meter above the ground. Simulate it falling.

**Questions:**
- How long does it take to hit the ground? (Use physics: distance = 0.5 × gravity × time²)
- What happens if you set gravity to -20 m/s²? (Double gravity)

### Exercise 2: Sensor Visualization

Add a LiDAR sensor to your robot in Gazebo. Place obstacles around it.

**Task:** Use `ros2 topic echo /scan` to see the distance readings. Move the obstacles and observe how the readings change.

### Exercise 3: Friction Experiment

Create two ramps: one with friction 0.1 (ice), one with friction 0.9 (rubber). Place identical cubes at the top.

**Question:** Which cube slides down? How does this affect robot wheel traction?

### Exercise 4: Sim-to-Real Thinking

You train a robot in simulation to stack blocks. When deployed to real hardware, the blocks often fall.

**Question:** What sim-to-real factors might cause this? List at least 3.

**Hint:** Consider sensors, physics, and environment differences.

---

## Common Mistakes Beginners Make

1. **Ignoring mass/inertia**: Setting all links to 1kg creates unrealistic behavior
2. **Skipping collision geometry**: Robot "ghosts" through walls
3. **Too-fast physics timesteps**: Simulation explodes or becomes unstable
4. **Not adding sensor noise**: Overfit to perfect simulated data
5. **Testing only in sim**: Always validate on real hardware eventually

---

## What's Next?

In the next chapters, we'll:

- Build a complete URDF model for a humanoid robot
- Implement navigation using simulated LiDAR in Gazebo
- Integrate Unity for training vision-based manipulation
- Deploy a sim-trained model to a real robot (Jetson Orin)

You now understand *why* and *how* simulation works. Next, we apply it to humanoid robotics!

---

**Resources for Deeper Learning:**

- [Gazebo Documentation](https://gazebosim.org/docs)
- [SDF Format Specification](http://sdformat.org/)
- [Unity Robotics Hub](https://github.com/Unity-Technologies/Unity-Robotics-Hub)
- [Sim-to-Real Transfer (Research Paper)](https://arxiv.org/abs/1703.06907)
