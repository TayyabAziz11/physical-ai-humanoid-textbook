"use strict";(globalThis.webpackChunkphysical_ai_humanoid_textbook=globalThis.webpackChunkphysical_ai_humanoid_textbook||[]).push([[661],{2781:(e,n,i)=>{i.d(n,{A:()=>o});var s=i(6540);const t={container:"container_rfuW",buttonGroup:"buttonGroup_PlP3",actionButton:"actionButton_Uyam",banner:"banner_o2gZ",bannerIcon:"bannerIcon_OknK",bannerText:"bannerText_QUdu",closeBanner:"closeBanner_dCt7"};var r=i(4848);function o({className:e}){const[n,i]=(0,s.useState)(!1),[o,a]=(0,s.useState)(!1);return(0,r.jsxs)("div",{className:`${t.container} ${e||""}`,children:[(0,r.jsxs)("div",{className:t.buttonGroup,children:[(0,r.jsx)("button",{className:t.actionButton,onClick:()=>{i(!0),a(!1)},"aria-label":"Personalize chapter content",children:"\u2728 Personalize for Me"}),(0,r.jsx)("button",{className:t.actionButton,onClick:()=>{a(!0),i(!1)},"aria-label":"View chapter in Urdu",children:"\ud83c\udf10 View in Urdu"})]}),n&&(0,r.jsxs)("div",{className:t.banner,role:"alert",children:[(0,r.jsx)("span",{className:t.bannerIcon,children:"\u2139\ufe0f"}),(0,r.jsx)("span",{className:t.bannerText,children:"Personalization coming soon \u2013 AI will adapt this chapter to your learning style and background"}),(0,r.jsx)("button",{className:t.closeBanner,onClick:()=>i(!1),"aria-label":"Close banner",children:"\u2715"})]}),o&&(0,r.jsxs)("div",{className:t.banner,role:"alert",children:[(0,r.jsx)("span",{className:t.bannerIcon,children:"\u2139\ufe0f"}),(0,r.jsx)("span",{className:t.bannerText,children:"Urdu translation coming soon \u2013 Full textbook will be available in \u0627\u0631\u062f\u0648"}),(0,r.jsx)("button",{className:t.closeBanner,onClick:()=>a(!1),"aria-label":"Close banner",children:"\u2715"})]})]})}},5442:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>p,frontMatter:()=>a,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"module-4-vision-language-action/chapter-1-vla-intro","title":"Chapter 1 - Introduction to VLA","description":"Introduction: The Convergence of AI and Robotics","source":"@site/docs/module-4-vision-language-action/chapter-1-vla-intro.mdx","sourceDirName":"module-4-vision-language-action","slug":"/module-4-vision-language-action/chapter-1-vla-intro","permalink":"/physical-ai-humanoid-textbook/docs/module-4-vision-language-action/chapter-1-vla-intro","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"chapter-1-vla-intro","title":"Chapter 1 - Introduction to VLA","sidebar_label":"Chapter 1: VLA Intro"},"sidebar":"tutorialSidebar","previous":{"title":"Overview","permalink":"/physical-ai-humanoid-textbook/docs/module-4-vision-language-action/overview"}}');var t=i(4848),r=i(8453),o=i(2781);const a={id:"chapter-1-vla-intro",title:"Chapter 1 - Introduction to VLA",sidebar_label:"Chapter 1: VLA Intro"},l="Chapter 1: Introduction to Vision-Language-Action",c={},d=[{value:"Introduction: The Convergence of AI and Robotics",id:"introduction-the-convergence-of-ai-and-robotics",level:2},{value:"What Are VLA Models?",id:"what-are-vla-models",level:2},{value:"The Traditional Pipeline (Old Way)",id:"the-traditional-pipeline-old-way",level:3},{value:"The VLA Pipeline (New Way)",id:"the-vla-pipeline-new-way",level:3},{value:"Key VLA Architectures",id:"key-vla-architectures",level:2},{value:"1. RT-1 (Robotics Transformer 1)",id:"1-rt-1-robotics-transformer-1",level:3},{value:"2. RT-2 (Robotics Transformer 2)",id:"2-rt-2-robotics-transformer-2",level:3},{value:"How VLA Models Work: Step-by-Step",id:"how-vla-models-work-step-by-step",level:2},{value:"Example Task: &quot;Pick up the red mug&quot;",id:"example-task-pick-up-the-red-mug",level:3},{value:"Training VLA Models: Imitation Learning",id:"training-vla-models-imitation-learning",level:2},{value:"The Training Process",id:"the-training-process",level:3},{value:"Datasets for VLA Training",id:"datasets-for-vla-training",level:3},{value:"Bridging VLA to ROS 2",id:"bridging-vla-to-ros-2",level:2},{value:"Example: Deploying RT-2 on a Robot",id:"example-deploying-rt-2-on-a-robot",level:3},{value:"Voice \u2192 Vision \u2192 Action: A Complete Example",id:"voice--vision--action-a-complete-example",level:2},{value:"System Architecture",id:"system-architecture",level:3},{value:"Step 1: Speech-to-Text with Whisper",id:"step-1-speech-to-text-with-whisper",level:3},{value:"Step 2: Launch the Full System",id:"step-2-launch-the-full-system",level:3},{value:"Step 3: Test It",id:"step-3-test-it",level:3},{value:"Capstone: Building an Autonomous Humanoid Assistant",id:"capstone-building-an-autonomous-humanoid-assistant",level:2},{value:"System Requirements",id:"system-requirements",level:3},{value:"Software Architecture",id:"software-architecture",level:3},{value:"Implementation Flow",id:"implementation-flow",level:3},{value:"Safety and Human Interaction",id:"safety-and-human-interaction",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Hands-On Practice",id:"hands-on-practice",level:2},{value:"Exercise 1: Calculate VLA Inference Speed",id:"exercise-1-calculate-vla-inference-speed",level:3},{value:"Exercise 2: Design a VLA Training Dataset",id:"exercise-2-design-a-vla-training-dataset",level:3},{value:"Exercise 3: Hybrid Planning",id:"exercise-3-hybrid-planning",level:3},{value:"Exercise 4: Safety Failure Modes",id:"exercise-4-safety-failure-modes",level:3},{value:"Common Mistakes Beginners Make",id:"common-mistakes-beginners-make",level:2},{value:"What&#39;s Next?",id:"whats-next",level:2}];function h(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"chapter-1-introduction-to-vision-language-action",children:"Chapter 1: Introduction to Vision-Language-Action"})}),"\n",(0,t.jsx)(o.A,{}),"\n",(0,t.jsx)(n.h2,{id:"introduction-the-convergence-of-ai-and-robotics",children:"Introduction: The Convergence of AI and Robotics"}),"\n",(0,t.jsx)(n.p,{children:'Imagine telling a humanoid robot: "Please bring me the red cup from the kitchen counter."'}),"\n",(0,t.jsx)(n.p,{children:"For this simple request to work, the robot needs to:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Understand language"}),': Parse "bring me the red cup from the kitchen counter"']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"See the world"}),": Identify objects, navigate spaces, avoid obstacles"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Take action"}),": Plan a path, grasp the cup, carry it safely"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Traditional robotics separates these capabilities into independent systems:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Speech recognition \u2192 Natural language processing"}),"\n",(0,t.jsx)(n.li,{children:"Computer vision \u2192 Object detection"}),"\n",(0,t.jsx)(n.li,{children:"Motion planning \u2192 Control systems"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Each system is built and trained independently, then duct-taped together with brittle integration code."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Vision-Language-Action (VLA)"})," models represent a paradigm shift: a ",(0,t.jsx)(n.strong,{children:"single neural network"})," that directly maps from pixels and text to robot actions."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Why This Matters:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"End-to-End Learning"}),": Train the whole pipeline together, not separate parts"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Emergent Behaviors"}),": The model learns shortcuts humans never explicitly programmed"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Data Efficiency"}),": Leverage internet-scale language and vision data for robotics"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Generalization"}),": One model works across many tasks and environments"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"In this chapter, you'll learn how VLA models like Google's RT-1 and RT-2 are transforming robots from scripted machines into adaptive agents."}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"what-are-vla-models",children:"What Are VLA Models?"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Vision-Language-Action (VLA)"})," models are neural networks that take ",(0,t.jsx)(n.strong,{children:"visual observations"})," (camera images) and ",(0,t.jsx)(n.strong,{children:"language instructions"})," (text commands) as input, and output ",(0,t.jsx)(n.strong,{children:"robot actions"})," (joint positions, gripper states)."]}),"\n",(0,t.jsx)(n.h3,{id:"the-traditional-pipeline-old-way",children:"The Traditional Pipeline (Old Way)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'User: "Pick up the apple"\n    \u2193\nSpeech-to-Text (Whisper) \u2192 "pick up the apple"\n    \u2193\nNLP (GPT-4) \u2192 Intent: PICK_OBJECT, Target: "apple"\n    \u2193\nObject Detection (YOLO) \u2192 Bounding box: [x=120, y=340, w=80, h=100]\n    \u2193\nMotion Planning (MoveIt) \u2192 Joint trajectory: [\u03b8\u2081, \u03b8\u2082, ..., \u03b8\u2087]\n    \u2193\nRobot executes motion\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Problems:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Each component fails independently (detection misses, planner gets stuck)"}),"\n",(0,t.jsx)(n.li,{children:"Errors compound through the pipeline"}),"\n",(0,t.jsx)(n.li,{children:"No shared learning\u2014vision doesn't help planning, and vice versa"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"the-vla-pipeline-new-way",children:"The VLA Pipeline (New Way)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'User: "Pick up the apple"\n    \u2193\nVLA Model (RT-2)\n  Inputs: Camera image + "pick up the apple"\n  Output: [Joint deltas: \u0394\u03b8\u2081, \u0394\u03b8\u2082, ..., \u0394\u03b8\u2087, gripper_open: False]\n    \u2193\nRobot executes action directly\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Advantages:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Single model"})," learns the entire mapping"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"End-to-end optimization"})," minimizes final task error, not intermediate metrics"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Implicit reasoning"}),' emerges (e.g., "avoid the obstacle" without explicit obstacle detection)']}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"key-vla-architectures",children:"Key VLA Architectures"}),"\n",(0,t.jsx)(n.h3,{id:"1-rt-1-robotics-transformer-1",children:"1. RT-1 (Robotics Transformer 1)"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Released by Google Research (2022)"})}),"\n",(0,t.jsx)(n.p,{children:"RT-1 is a transformer-based model trained on 130,000 robot demonstrations collected in real kitchens and offices."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Architecture:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Camera Images (6 views)             \u2502  \u2190 Visual input\n\u2502  [224x224x3 per camera]             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2193\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  EfficientNet \u2502  \u2190 Image encoder\n    \u2502  (pretrained) \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2193\n    [Visual tokens: 512-dim]\n            \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Language Instruction                \u2502  \u2190 Language input\n\u2502  "Pick up the blue block"            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2193\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  USE Encoder  \u2502  \u2190 Universal Sentence Encoder\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2193\n    [Language tokens: 512-dim]\n            \u2193\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  Transformer Decoder    \u2502  \u2190 Attention mechanism\n    \u2502  (8 layers, 128 heads)  \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2193\n    [Action tokens: 11-dim]\n                \u2193\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  Robot Actions (every 3 Hz)      \u2502\n    \u2502  [x, y, z, roll, pitch, yaw,     \u2502\n    \u2502   gripper, terminate]            \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Training Data:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"130,000 episodes of robot manipulation tasks"}),"\n",(0,t.jsx)(n.li,{children:'Tasks: "Pick X", "Move X to Y", "Open drawer", "Close lid"'}),"\n",(0,t.jsx)(n.li,{children:"Collected using human teleoperation (joystick control)"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Performance:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"97% success on trained tasks"}),"\n",(0,t.jsx)(n.li,{children:"76% success on novel objects (unseen during training)"}),"\n",(0,t.jsx)(n.li,{children:"Runs at 3 Hz (333 ms per action) on a single GPU"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"2-rt-2-robotics-transformer-2",children:"2. RT-2 (Robotics Transformer 2)"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Released by Google Research (2023)"})}),"\n",(0,t.jsxs)(n.p,{children:["RT-2 extends RT-1 by using ",(0,t.jsx)(n.strong,{children:"vision-language models (VLMs)"})," like PaLM-E and CLIP, which are pretrained on ",(0,t.jsx)(n.strong,{children:"internet-scale"})," image-text data."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Key Insight:"})}),"\n",(0,t.jsx)(n.p,{children:"Internet data contains implicit physics and common-sense reasoning:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'Images of "pouring water" teach liquid dynamics'}),"\n",(0,t.jsx)(n.li,{children:'"Open the jar" captions teach twist motions'}),"\n",(0,t.jsx)(n.li,{children:'"Move the chair" images teach large-object manipulation'}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["RT-2 ",(0,t.jsx)(n.strong,{children:"fine-tunes"})," these VLMs on robot data, transferring web knowledge to robotics."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Architecture:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'Pretrained VLM (PaLM-E / CLIP)\n    \u2193 Fine-tune on robot data\nRT-2 Model\n    \u2193 Input: Image + "pour water into cup"\n    \u2193 Output: [\u0394x, \u0394y, \u0394z, gripper_force]\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Performance Improvements Over RT-1:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Emergent skills"}),': Can perform tasks never seen in robot data (e.g., "move banana to Taylor Swift" \u2192 moves banana to a Taylor Swift poster)']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Symbol understanding"}),': Recognizes objects by brand names (e.g., "pick up the Coke can")']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Zero-shot generalization"}),": 62% success on completely novel tasks"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Example:"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Instruction:"}),' "Move the apple to the picture of a dog"']}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"What Happens:"})}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:'RT-2 uses CLIP to recognize "apple" and "dog picture"'}),"\n",(0,t.jsx)(n.li,{children:"Plans a motion to move the apple toward the dog"}),"\n",(0,t.jsx)(n.li,{children:"Executes the action with closed-loop control"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"No explicit object detection or motion planning code\u2014it's all learned!"}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"how-vla-models-work-step-by-step",children:"How VLA Models Work: Step-by-Step"}),"\n",(0,t.jsx)(n.p,{children:"Let's walk through how a VLA model processes a command."}),"\n",(0,t.jsx)(n.h3,{id:"example-task-pick-up-the-red-mug",children:'Example Task: "Pick up the red mug"'}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Step 1: Image Encoding"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Camera images (6 views from different angles)\nimages = [camera1.read(), camera2.read(), ..., camera6.read()]\n\n# Encode images using EfficientNet (pretrained on ImageNet)\nimage_encoder = EfficientNet()\nvisual_features = image_encoder(images)  # Shape: [6, 512]\n\n# Pool across cameras\nvisual_tokens = pool(visual_features)  # Shape: [512]\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Step 2: Language Encoding"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# User instruction\ninstruction = "pick up the red mug"\n\n# Encode with Universal Sentence Encoder\nlanguage_encoder = UniversalSentenceEncoder()\nlanguage_tokens = language_encoder(instruction)  # Shape: [512]\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Step 3: Transformer Fusion"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Concatenate vision and language\ninput_tokens = concat(visual_tokens, language_tokens)  # Shape: [1024]\n\n# Pass through Transformer\ntransformer = TransformerDecoder(layers=8, heads=128)\naction_logits = transformer(input_tokens)  # Shape: [11]\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Step 4: Action Decoding"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Action space: [x, y, z, roll, pitch, yaw, gripper, terminate]\naction = decode_action(action_logits)\n\n# Example output:\n# [\u0394x: +0.02m, \u0394y: -0.01m, \u0394z: +0.05m,  # Move toward mug\n#  roll: 0\xb0, pitch: 45\xb0, yaw: 0\xb0,       # Tilt gripper\n#  gripper: CLOSED, terminate: False]   # Close gripper, continue\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Step 5: Robot Execution"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Send action to ROS 2\njoint_state_msg = compute_inverse_kinematics(action)\njoint_pub.publish(joint_state_msg)\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Step 6: Repeat"})}),"\n",(0,t.jsx)(n.p,{children:"The model runs at 3 Hz, continuously adjusting based on new camera frames until the task completes."}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"training-vla-models-imitation-learning",children:"Training VLA Models: Imitation Learning"}),"\n",(0,t.jsxs)(n.p,{children:["VLA models are trained using ",(0,t.jsx)(n.strong,{children:"imitation learning"}),": learning from expert demonstrations."]}),"\n",(0,t.jsx)(n.h3,{id:"the-training-process",children:"The Training Process"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Step 1: Collect Demonstrations"})}),"\n",(0,t.jsx)(n.p,{children:"Humans teleoperate a robot to complete tasks while recording:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Camera images at each timestep"}),"\n",(0,t.jsx)(n.li,{children:'Language instructions (e.g., "pick up the red block")'}),"\n",(0,t.jsx)(n.li,{children:"Robot actions (joint positions, gripper state)"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Step 2: Create a Dataset"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Example dataset entry\n{\n    "instruction": "pick up the red block",\n    "images": [img_t0, img_t1, ..., img_t100],  # 100 frames @ 3 Hz = 33 seconds\n    "actions": [a_t0, a_t1, ..., a_t100],       # Corresponding actions\n}\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Step 3: Train with Behavioral Cloning"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Loss function: Predict expert actions\nloss = MSE(predicted_actions, expert_actions)\n\n# Training loop\nfor batch in dataset:\n    images, instructions, expert_actions = batch\n    predicted_actions = vla_model(images, instructions)\n    loss = criterion(predicted_actions, expert_actions)\n    loss.backward()\n    optimizer.step()\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Step 4: Fine-Tune on Robot"})}),"\n",(0,t.jsxs)(n.p,{children:["Deploy the model on the robot and collect ",(0,t.jsx)(n.strong,{children:"on-policy"})," data (data from the learned policy, not humans). This corrects distribution shift."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# DAgger (Dataset Aggregation) algorithm\n1. Deploy current policy \u03c0\n2. When policy fails, human corrects action\n3. Add (state, corrected_action) to dataset\n4. Retrain policy\n5. Repeat\n"})}),"\n",(0,t.jsx)(n.h3,{id:"datasets-for-vla-training",children:"Datasets for VLA Training"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Dataset"}),(0,t.jsx)(n.th,{children:"Size"}),(0,t.jsx)(n.th,{children:"Tasks"}),(0,t.jsx)(n.th,{children:"Source"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"RT-1 Dataset"})}),(0,t.jsx)(n.td,{children:"130K demos"}),(0,t.jsx)(n.td,{children:"Pick-and-place, drawer opening"}),(0,t.jsx)(n.td,{children:"Google Robot Kitchens"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Open-X Embodiment"})}),(0,t.jsx)(n.td,{children:"1M+ demos"}),(0,t.jsx)(n.td,{children:"160+ tasks, 22 robot types"}),(0,t.jsx)(n.td,{children:"Multi-institution collaboration"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"CALVIN"})}),(0,t.jsx)(n.td,{children:"24K demos"}),(0,t.jsx)(n.td,{children:"Long-horizon tasks (34 steps avg)"}),(0,t.jsx)(n.td,{children:"Simulation (table-top)"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"RoboTurk"})}),(0,t.jsx)(n.td,{children:"100K demos"}),(0,t.jsx)(n.td,{children:"Manipulation tasks"}),(0,t.jsx)(n.td,{children:"Simulated + Real"})]})]})]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Open-X Embodiment"})," (2023) is the largest: combines data from 22 different robot platforms, enabling ",(0,t.jsx)(n.strong,{children:"cross-embodiment learning"})," (train on one robot, transfer to another)."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"bridging-vla-to-ros-2",children:"Bridging VLA to ROS 2"}),"\n",(0,t.jsx)(n.p,{children:"VLA models are typically trained in Python (PyTorch/TensorFlow). Here's how to integrate them into a ROS 2 system."}),"\n",(0,t.jsx)(n.h3,{id:"example-deploying-rt-2-on-a-robot",children:"Example: Deploying RT-2 on a Robot"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Architecture:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"ROS 2 Node: VLA Control\n    \u2193 Subscribes to: /camera/image, /speech/transcript\n    \u2193 Publishes to: /joint_commands\n    \u2193 Uses: RT-2 PyTorch model\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Implementation:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, JointState\nfrom std_msgs.msg import String\nimport torch\nfrom rt2_model import RT2  # Hypothetical RT-2 implementation\n\nclass VLAControlNode(Node):\n    def __init__(self):\n        super().__init__('vla_control_node')\n\n        # Load RT-2 model\n        self.model = RT2.from_pretrained('google/rt-2-base')\n        self.model.eval()  # Inference mode\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.model.to(self.device)\n\n        # Subscribe to camera and language input\n        self.create_subscription(Image, '/camera/image', self.image_callback, 10)\n        self.create_subscription(String, '/speech/transcript', self.command_callback, 10)\n\n        # Publisher for robot actions\n        self.action_pub = self.create_publisher(JointState, '/joint_commands', 10)\n\n        # State\n        self.current_image = None\n        self.current_instruction = None\n\n        # Control loop at 3 Hz (RT-2 inference rate)\n        self.create_timer(1.0 / 3.0, self.control_loop)\n\n        self.get_logger().info('VLA Control Node ready')\n\n    def image_callback(self, msg):\n        # Convert ROS Image to numpy array\n        self.current_image = self.ros_img_to_numpy(msg)\n\n    def command_callback(self, msg):\n        self.current_instruction = msg.data\n        self.get_logger().info(f'New instruction: {self.current_instruction}')\n\n    def control_loop(self):\n        if self.current_image is None or self.current_instruction is None:\n            return  # Wait for both inputs\n\n        # Prepare inputs\n        image_tensor = self.preprocess_image(self.current_image)\n        instruction_tensor = self.tokenize_instruction(self.current_instruction)\n\n        # Run VLA model\n        with torch.no_grad():\n            action = self.model(image_tensor, instruction_tensor)\n\n        # Convert action to JointState message\n        joint_msg = self.action_to_joint_state(action)\n        self.action_pub.publish(joint_msg)\n\n        self.get_logger().info(f'Published action: {action.tolist()}')\n\n    def preprocess_image(self, img):\n        # Resize to 224x224, normalize, convert to tensor\n        img_resized = cv2.resize(img, (224, 224))\n        img_tensor = torch.from_numpy(img_resized).permute(2, 0, 1).float() / 255.0\n        img_tensor = img_tensor.unsqueeze(0).to(self.device)\n        return img_tensor\n\n    def tokenize_instruction(self, text):\n        # Use model's tokenizer (e.g., SentencePiece)\n        tokens = self.model.tokenizer(text, return_tensors='pt').to(self.device)\n        return tokens\n\n    def action_to_joint_state(self, action):\n        # Convert VLA action [\u0394x, \u0394y, \u0394z, ...] to joint positions\n        # This requires inverse kinematics (IK)\n        joint_positions = self.compute_ik(action)\n\n        joint_msg = JointState()\n        joint_msg.name = ['joint1', 'joint2', 'joint3', 'joint4', 'joint5', 'joint6', 'gripper']\n        joint_msg.position = joint_positions.tolist()\n\n        return joint_msg\n\n    def compute_ik(self, action):\n        # Placeholder: Use a library like PyBullet or MoveIt for IK\n        # action format: [\u0394x, \u0394y, \u0394z, roll, pitch, yaw, gripper_state]\n        # Output: joint angles\n        return torch.rand(7)  # Dummy values\n\n    def ros_img_to_numpy(self, msg):\n        # Convert ROS Image to OpenCV format\n        import cv_bridge\n        bridge = cv_bridge.CvBridge()\n        return bridge.imgmsg_to_cv2(msg, desired_encoding='rgb8')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VLAControlNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"What This Node Does:"})}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Subscribes to camera images and voice commands"}),"\n",(0,t.jsx)(n.li,{children:"Runs RT-2 model at 3 Hz to predict actions"}),"\n",(0,t.jsx)(n.li,{children:"Converts actions to joint commands using inverse kinematics"}),"\n",(0,t.jsxs)(n.li,{children:["Publishes to ",(0,t.jsx)(n.code,{children:"/joint_commands"})," for robot execution"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"voice--vision--action-a-complete-example",children:"Voice \u2192 Vision \u2192 Action: A Complete Example"}),"\n",(0,t.jsx)(n.p,{children:"Let's build a complete pipeline where a user can speak commands to a robot."}),"\n",(0,t.jsx)(n.h3,{id:"system-architecture",children:"System Architecture"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Microphone \u2502 \u2192 /audio/raw\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Whisper Node \u2502 \u2192 /speech/transcript\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     /camera/image\n\u2502  VLA Node (RT-2) \u2502 \u2190 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2193\n   /joint_commands\n         \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Robot Hardware \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-1-speech-to-text-with-whisper",children:"Step 1: Speech-to-Text with Whisper"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom audio_msgs.msg import AudioData\nfrom std_msgs.msg import String\nimport whisper\n\nclass WhisperNode(Node):\n    def __init__(self):\n        super().__init__('whisper_node')\n\n        # Load Whisper model\n        self.model = whisper.load_model('base')  # 'tiny', 'base', 'small', 'medium', 'large'\n\n        # Subscribe to audio\n        self.create_subscription(AudioData, '/audio/raw', self.audio_callback, 10)\n\n        # Publisher for transcripts\n        self.transcript_pub = self.create_publisher(String, '/speech/transcript', 10)\n\n        self.get_logger().info('Whisper Node ready')\n\n    def audio_callback(self, msg):\n        # Convert audio to numpy array\n        audio_array = np.frombuffer(msg.data, dtype=np.float32)\n\n        # Transcribe with Whisper\n        result = self.model.transcribe(audio_array)\n        transcript = result['text']\n\n        self.get_logger().info(f'Transcript: {transcript}')\n\n        # Publish transcript\n        transcript_msg = String()\n        transcript_msg.data = transcript\n        self.transcript_pub.publish(transcript_msg)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-2-launch-the-full-system",children:"Step 2: Launch the Full System"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Launch camera node\nros2 run usb_cam usb_cam_node\n\n# Terminal 2: Launch Whisper node\nros2 run vla_control whisper_node\n\n# Terminal 3: Launch VLA control node\nros2 run vla_control vla_control_node\n\n# Terminal 4: Launch robot hardware interface\nros2 launch my_robot robot.launch.py\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-3-test-it",children:"Step 3: Test It"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# User speaks: "Pick up the red cup"\n# \u2192 Whisper transcribes to "/speech/transcript"\n# \u2192 VLA node receives image + transcript\n# \u2192 RT-2 predicts action\n# \u2192 Robot executes motion\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"capstone-building-an-autonomous-humanoid-assistant",children:"Capstone: Building an Autonomous Humanoid Assistant"}),"\n",(0,t.jsx)(n.p,{children:"Let's design a complete autonomous humanoid that can perform household tasks."}),"\n",(0,t.jsx)(n.h3,{id:"system-requirements",children:"System Requirements"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Capabilities:"})}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Voice command understanding"}),"\n",(0,t.jsx)(n.li,{children:"Object detection and tracking"}),"\n",(0,t.jsx)(n.li,{children:"Navigation in indoor environments"}),"\n",(0,t.jsx)(n.li,{children:"Manipulation (pick, place, open doors)"}),"\n",(0,t.jsx)(n.li,{children:"Human-robot interaction (gestures, facial expressions)"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Hardware:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Humanoid Platform"}),": Figure 02, Tesla Optimus, or Boston Dynamics Atlas"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sensors"}),": RGB-D cameras (2), LiDAR, IMU, microphones (4)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Compute"}),": NVIDIA Jetson AGX Orin (64 GB RAM, 275 TOPS)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Actuators"}),": 25 DOF (arms: 7 each, legs: 6 each, torso: 3, head: 2)"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"software-architecture",children:"Software Architecture"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502            High-Level Planning (GPT-4)           \u2502\n\u2502  Task: "Make me a sandwich"                     \u2502\n\u2502  \u2192 Subtasks: [Navigate to kitchen, open fridge, \u2502\n\u2502               grab bread, grab cheese, ...]     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         VLA Model (RT-2 / Custom)                \u2502\n\u2502  Subtask: "Grab bread"                          \u2502\n\u2502  Input: Camera + "grab bread"                   \u2502\n\u2502  Output: End-effector trajectory                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502          Isaac ROS (Perception)                  \u2502\n\u2502  - Object detection (bread, cheese, etc.)       \u2502\n\u2502  - Visual SLAM (localization)                   \u2502\n\u2502  - Depth estimation (obstacle avoidance)        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502          ROS 2 Control (MoveIt 2)                \u2502\n\u2502  - Path planning (collision-free)               \u2502\n\u2502  - Inverse kinematics                           \u2502\n\u2502  - Joint trajectory execution                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502          Robot Hardware                          \u2502\n\u2502  - Actuators (motors)                           \u2502\n\u2502  - Sensors (encoders, force/torque)             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n'})}),"\n",(0,t.jsx)(n.h3,{id:"implementation-flow",children:"Implementation Flow"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Step 1: High-Level Planning with GPT-4"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import openai\n\ndef plan_task(task):\n    prompt = f"""\n    You are a robot task planner. Decompose this task into subtasks:\n    Task: {task}\n\n    Output format (JSON):\n    {{"subtasks": ["subtask1", "subtask2", ...]}}\n    """\n\n    response = openai.ChatCompletion.create(\n        model="gpt-4",\n        messages=[{"role": "user", "content": prompt}]\n    )\n\n    plan = json.loads(response.choices[0].message.content)\n    return plan[\'subtasks\']\n\n# Example\ntask = "Make me a sandwich"\nsubtasks = plan_task(task)\n# Output: ["Navigate to kitchen", "Open fridge", "Grab bread", "Grab cheese", "Assemble sandwich"]\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Step 2: Execute Each Subtask with VLA"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"for subtask in subtasks:\n    # Publish subtask as instruction\n    instruction_msg = String()\n    instruction_msg.data = subtask\n    instruction_pub.publish(instruction_msg)\n\n    # VLA node picks this up and executes\n    # Wait for completion signal\n    wait_for_completion()\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Step 3: VLA Execution (RT-2)"})}),"\n",(0,t.jsx)(n.p,{children:"The VLA node (from earlier) receives each subtask and executes it:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# VLA Node receives: "Grab bread"\n# \u2192 Sees environment via camera\n# \u2192 Predicts action to move toward bread\n# \u2192 Publishes joint commands\n# \u2192 Robot grabs bread\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Step 4: Perception with Isaac ROS"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Isaac ROS node detects "bread" in the scene\n# Publishes object location to /detected_objects\n# VLA node uses this to refine actions\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Step 5: Motion Planning with MoveIt 2"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# MoveIt 2 plans collision-free trajectory\n# Publishes to /joint_trajectory_controller\n# Robot executes motion smoothly\n"})}),"\n",(0,t.jsx)(n.h3,{id:"safety-and-human-interaction",children:"Safety and Human Interaction"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Key Safety Features:"})}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Force/Torque Limits"}),": Motors cut power if force exceeds 50N"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Emergency Stop"}),": Wireless e-stop button halts all motion"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Vision-Based Monitoring"}),": Detects humans in workspace and slows down"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Compliance Control"}),": Soft joints that yield on unexpected contact"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Human-Robot Interaction:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Detect human gestures (e.g., "stop" hand sign)\ngesture = detect_gesture(camera_image)\n\nif gesture == "stop":\n    robot.stop_all_motion()\n    speak("Stopping immediately")\n\n# Facial expressions for feedback\nif task_success:\n    robot.display_emotion("happy")\nelse:\n    robot.display_emotion("confused")\n    speak("I\'m not sure what to do. Can you help?")\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"VLA models"})," unify vision, language, and action into a single end-to-end learnable system"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"RT-1"})," demonstrated imitation learning on 130K real-world robot demos"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"RT-2"})," leverages internet-scale vision-language models for emergent generalization"]}),"\n",(0,t.jsxs)(n.li,{children:["VLA models integrate seamlessly with ",(0,t.jsx)(n.strong,{children:"ROS 2"})," as control nodes"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Voice \u2192 Vision \u2192 Action"})," pipelines enable natural human-robot interaction"]}),"\n",(0,t.jsxs)(n.li,{children:["A complete autonomous humanoid combines ",(0,t.jsx)(n.strong,{children:"VLA"}),", ",(0,t.jsx)(n.strong,{children:"Isaac ROS"}),", ",(0,t.jsx)(n.strong,{children:"MoveIt 2"}),", and ",(0,t.jsx)(n.strong,{children:"GPT-4"})]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"hands-on-practice",children:"Hands-On Practice"}),"\n",(0,t.jsx)(n.h3,{id:"exercise-1-calculate-vla-inference-speed",children:"Exercise 1: Calculate VLA Inference Speed"}),"\n",(0,t.jsx)(n.p,{children:"If RT-2 runs at 3 Hz (333 ms per action), and a task requires 100 actions to complete:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"How long does the task take?"}),"\n",(0,t.jsx)(n.li,{children:"If you upgrade to a GPU that's 2x faster, what's the new task completion time?"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"exercise-2-design-a-vla-training-dataset",children:"Exercise 2: Design a VLA Training Dataset"}),"\n",(0,t.jsx)(n.p,{children:"You want to train a VLA model to fold laundry. What demonstrations would you collect?"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Hint:"})," Think about variations (shirt sizes, fabric types, initial positions)."]}),"\n",(0,t.jsx)(n.h3,{id:"exercise-3-hybrid-planning",children:"Exercise 3: Hybrid Planning"}),"\n",(0,t.jsx)(n.p,{children:'For the task "Make me a sandwich," which subtasks should use:'}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"GPT-4 planning"})," (high-level)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"VLA execution"})," (low-level)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Classical control"})," (precise motions)"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Example Answer:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'GPT-4: "Navigate to kitchen" (high-level)'}),"\n",(0,t.jsx)(n.li,{children:'VLA: "Grab bread" (manipulation)'}),"\n",(0,t.jsx)(n.li,{children:'Classical: "Spread butter at 45\xb0 angle" (precise force control)'}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"exercise-4-safety-failure-modes",children:"Exercise 4: Safety Failure Modes"}),"\n",(0,t.jsx)(n.p,{children:"A humanoid is serving drinks in a restaurant. List 3 failure modes and mitigation strategies."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Hint:"})," Consider spills, collisions, and misunderstandings."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"common-mistakes-beginners-make",children:"Common Mistakes Beginners Make"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Training only on successful demos"}),": Include failure cases to teach error recovery"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Ignoring action frequency"}),": VLA models expect consistent control rates (e.g., 3 Hz)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Overfitting to demo environments"}),": Use domain randomization (lighting, backgrounds)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Skipping human feedback"}),": Fine-tune with corrective demonstrations (DAgger)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Not testing on real hardware early"}),": Simulation-to-real gap exists even for VLAs"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"whats-next",children:"What's Next?"}),"\n",(0,t.jsx)(n.p,{children:"In the next chapter, we'll:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Train a custom VLA model from scratch"}),"\n",(0,t.jsx)(n.li,{children:"Collect demonstrations using teleoperation"}),"\n",(0,t.jsx)(n.li,{children:"Fine-tune RT-2 on a new task"}),"\n",(0,t.jsx)(n.li,{children:"Deploy the model on a real robot with ROS 2"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"You've now mastered the VLA paradigm. Next, we'll build a production-ready system!"}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Resources for Deeper Learning:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://robotics-transformer.github.io/",children:"RT-1 Paper"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/",children:"RT-2 Paper"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://robotics-transformer-x.github.io/",children:"Open-X Embodiment Dataset"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://github.com/openai/whisper",children:"Whisper (OpenAI Speech Recognition)"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://control.ros.org/master/index.html",children:"ROS 2 Control Tutorials"})}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var s=i(6540);const t={},r=s.createContext(t);function o(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);